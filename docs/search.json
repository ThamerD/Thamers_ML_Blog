[
  {
    "objectID": "posts/guide/guide.html",
    "href": "posts/guide/guide.html",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "",
    "text": "Welcome to the beginner’s guide to Supervised Machine Learning!\nIn this guide, I will walk you through complete workflow for using supervised machine learning to solve a real-world problem. Keep in mind that Supervised Machine Learning is a vast sea of knowledge that will take more than this humble little guide to fully master. However, after reading through (and following along!) this guide, I believe you will have built a solid foundation that will make you feel confident about approaching future problems using ML.\nFigure 1: A set of questions to guide you through the ML workflow."
  },
  {
    "objectID": "posts/guide/guide.html#problem-definition",
    "href": "posts/guide/guide.html#problem-definition",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "1. Problem Definition",
    "text": "1. Problem Definition\nBefore diving into data cleaning and modeling, we first have to validate whether ot not our problem is actually an ML problem. Generally, if you answer is yes to the following questions, then you have a ML problem:\n\nDoes the problem require reaching a certain output based on a set of inputs?\n\nIf not, it is unlikely that you will be able to build and train a ML model to achieve the desired result. Keep in mind that this doesn’t necessarily mean you should give up, but rather look for ways to reframe your perspective on the problem to make it more suitable for ML. For example, instead of asking “How can I increase the number of subscribers to my blog”, ask “What features of my blog are most associated with a change in the number of subscribers?”. These “Features” can be any data input you have access to that you believe may impact your result. If your problem is more general rather than personal, you may find the data your looking for in one of the plethora of open data source on the internet (e.g., https://www.kaggle.com/datasets).\n\nDo we have access to a reliable set of data that will help us reach our goal?\n\nIf not, you may attempt to manually collect the required data yourself (keep in mind that the quality of your results will depend greatly on the quality and size of your data). If you cannot find a way to access or collect the data, then I’m afraid no ML model will be able to help you.\n\nIs the data so large that a human being cannot effectively read it and use it to solve the problem?\n\nIf not, you are likely better off manually looking for patterns and using simple calculations to reach your goals. As ML models require a large amount of data to be effective.\n\n\nCongratulations! If you’ve reached this point that means you have already broken down your problem into a set of inputs (features) that influence a particular output (target). And you’ve determined a reliable source of data that will be used to train your model, and you’ve (roughly) determined that your data set is large enough to be used in ML."
  },
  {
    "objectID": "posts/guide/guide.html#preparing-our-workspace",
    "href": "posts/guide/guide.html#preparing-our-workspace",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "2. Preparing our workspace",
    "text": "2. Preparing our workspace\nIn this section, we simply want to mindfully prepare a workspace within which we will perform the majority of our analysis work. Although there are many different pieces of software that we can use for this, in this guide we will be using the Pandas package in the Python programming language using VS Code:\n\nInstall Python: https://www.python.org/downloads/\n\nPython is a programming language that houses numerous packages that contain handy tools that will help us perform our analysis. It is the most popular programming language for ML purposes.\nInstall the “conda” package to manage your environments.\n\nInstall VS Code: https://code.visualstudio.com/download\n\nVS Code is the world’s most popular integrated development environment (IDE).\nI also recommend you install the Jupyter extension within VS Code.\n\nCreate a folder that will store all of your analysis files.\nCreate a new conda environment to host the packages you will use for your analysis.\n\nInstall the “pandas” package for data manipulation.\ninstall the “sklearn” package which contains an assortment of ML models and utilities.\nInstall the “matplotlib” package for data visualization. (“altair” is a great option as well).\nFeel free to install any additional packages that you believe will be useful for your analysis."
  },
  {
    "objectID": "posts/guide/guide.html#data-collection",
    "href": "posts/guide/guide.html#data-collection",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "3. Data Collection",
    "text": "3. Data Collection\nAlthough we have already determined a source for our data, we still need to transform our data into a tabular structure that makes it easy to feed into an ML model. Here is a list of common data formats and some ways you can transform them into the structure we need:\n\nTabular Data (.CSV, .TSV, .XLS, .XLSX, etc.) - No transformation need. You’re good to go into the next part!\n\nRelational Data (MySQL, PostgreSQL, etc.) - Use SQL connector libraries such as “mysql-connector-python” for MySQL or “psycopg2” for PostgreSQL.\n\nNon-relational Data (NoSQL, MongoDB, etc.) - Use NoSQL connector libraries such as “pymongo” for MongoDB.\n\nMedia (Images, Videos, Audio) - This type of data is a bit tricky and has to be handled differently. For now it is out of the scope of this guide.\n\nSample code :\nimport pandas as pd\n\nraw_data = \"data/2023_Property_Tax_Assessment.csv\"\nhousing_df = pd.read_csv(raw_data)"
  },
  {
    "objectID": "posts/guide/guide.html#data-cleaning-and-splitting",
    "href": "posts/guide/guide.html#data-cleaning-and-splitting",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "4. Data Cleaning and Splitting",
    "text": "4. Data Cleaning and Splitting\n\nEnsure your data is tidy:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nSplit your data into training and test sets.\n\nA generally good split is 80% training data and 20% test data.\nDepending on the size of your data you may need to adjust the split to have more accurate test results.\n\n\nSample code :\nfrom sklearn.model_selection import train_test_split\n\ntidy_df = df.melt(id_vars=\"Name\", var_name=\"Subject\", value_name=\"Score\")\n\n# Splitting our tidy data into training and test data\ntrain_df, test_df = train_test_split(tidy_df, test_size=0.3, random_state=123)"
  },
  {
    "objectID": "posts/guide/guide.html#data-exploration",
    "href": "posts/guide/guide.html#data-exploration",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "5. Data Exploration",
    "text": "5. Data Exploration\n\nLook at the head and tail of your dataset.\nLook at the types, minimums, maximums, means, and medians of your features. (tip: you can use “describe()” to quickly analyze such metrics).\nUse visualization libraries such as “matplotlib” and “altair” to look at how your data is distributed.\n\nThis will give you an idea of what to expect from your data and you will be able to see any patters or potential issues from the distribution.\n\n\nSample code :\nimport altair as alt\n\n# Create a histogram for assessment values\nhistogram = alt.Chart(housing_df).mark_bar().encode(\n        alt.X(\"assess_2022:Q\", bin=alt.Bin(maxbins=2000), title=\"Assessment Value\").scale(domain=(0, 2000000), clamp=True),\n        alt.Y(\"count():Q\", title=\"Frequency\"),\n    ).properties(\n        title=\"Distribution of House Assessment Values (2022)\"\n    )\n\nFigure 2: An example of exploratory data analysis using a histogram."
  },
  {
    "objectID": "posts/guide/guide.html#preprocessing-and-feature-engineering",
    "href": "posts/guide/guide.html#preprocessing-and-feature-engineering",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "6. Preprocessing and Feature Engineering",
    "text": "6. Preprocessing and Feature Engineering\n\nPreprocessing refers to the process of translating a feature into a different scale or form to make it easier for our model to understand it. Here are some basic preprocessing technique that you can use depending on a features data type:\n\nNumeric features: Standard Scaler.\nCategorical features: One-Hot Encoding.\nBinary features: Represent with 0 and 1.\nText features: Bag-of-Words if syntax doesn’t matter. NLTK if syntax does matter. (This one is tricky and has many different approaches. The best approach depends on your specific problem).\nDate and time features: Extract meaningful components (e.g., year, month, day, hour, day of week). (This is another tricky one that has plenty of approaches. Depending on your problem and model the best approach may change).\nKeep in mind that there are plenty more types of features and plenty more preprocessing techniques, these examples are only meant to serve as a starting point.\n\nFeature Engineering can be tricky so feel free to skip this step if this is your first time using ML. Essentially, feature engineering uses existing features and information to create new features that describe the same information from a different angle that can be more beneficial for our model. For example, you may use a “Date of Birth” feature to create a new feature “Age” which can be easier to work with since it is a simple numeric feature rather than a date.\n\nSample code :\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Divide features by type\ncategorical_features = ['garage', 'firepl', 'bsmt', 'bdevl']\nnumeric_features = ['meters']\n\n# Create the column transformer to preprocess features\npreprocessor = make_column_transformer(\n    (OneHotEncoder(), categorical_features),  # One-hot encode categorical columns\n    (StandardScaler(), numeric_features),  # Standardize numeric columns\n)"
  },
  {
    "objectID": "posts/guide/guide.html#feature-selection-and-model-building",
    "href": "posts/guide/guide.html#feature-selection-and-model-building",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "7. Feature Selection and Model Building",
    "text": "7. Feature Selection and Model Building\n\nAlthough we generally like feeding our model as much data as possible, more features doesn’t necessarily result in better performance. In fact, a large number of features can confuse our model and cause “overfitting” which will worsen its performance.\n\nWe have to be selective about which features to keep.\nStart by removing unique features such as IDs, phone number, etc. We want to remove these because they are unlikely to contain any valuable patterns that will benefit our model.\nRemove any other features that you think will have little or no impact on our target\nDon’t worry, you will have the opportunity to come back later and bring back or remove some more features to see how they impact your model.\n\nNow is a good time to do some research on what ML models generally work well for your type of problem. Here is a short list of well known models and when to use them:\n\nLinear Regression\n\nUse for: Predicting a continuous numerical value.\nExample: Predicting house prices based on its size, location, and number of bedrooms.\n\nLogistic Regression\n\nUse for: Binary classification problems.\nExample: Predicting whether a customer will churn.\n\nDecision Trees\n\nUse for: Interpretable models for classification or regression.\nExample: Classifying loan approvals based on credit scores, income, etc.\n\nRandom Forest\n\nUse for: Handling complex classification and regression tasks with reduced overfitting.\nExample: Predicting customer segments for marketing campaigns.\n\nGradient Boosting (e.g., XGBoost, LightGBM, CatBoost)\n\nUse for: Complex non-linear relationships and feature interactions.\nExample: Predicting loan default probabilities or rankings.\n\nSupport Vector Machines (SVM)\n\nUse for: Classification problems, especially with small or high-dimensional datasets.\nExample: Classifying emails as spam or non-spam.\n\nK-Nearest Neighbors (KNN)\n\nUse for: Simple classification or regression problems.\nExample: Recommending products based on user similarity.\n\nNaive Bayes\n\nUse for: Text classification with large datasets.\nExample: Classifying sentiment in customer reviews.\n\n\nIf you feel indecisive about which one to use, fret not, it is totally valid to find the best model for your problem using trial and error.\nKeep in mind that you can also use an “Ensemble” of a number of these models. But this is a more advanced technique that we will save for another time.\nYou also want to create a “dummy” model to act as your baseline to help you gauge your model. Examples include “DummyClassifier” and “DummyRegressor”.\n\nSample code :\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.pipeline import make_pipeline\n\nridge_pipeline = make_pipeline(preprocessor, RidgeCV())\ndummy_pipe = make_pipeline(preprocessor, DummyRegressor())"
  },
  {
    "objectID": "posts/guide/guide.html#evaluation-and-model-selection",
    "href": "posts/guide/guide.html#evaluation-and-model-selection",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "8. Evaluation and Model Selection",
    "text": "8. Evaluation and Model Selection\n\nUse cross-validation to evaluate the model you created.\nGo back and redo your feature selection and preprocessing to see how it affects your models performance.\nTry different models and compare them against each other.\nContinuously iterate through this process until you achieve a satisfactory validation score.\n\nSample code :\nfrom sklearn.model_selection import cross_validate\n\n# initialize results dictionary\ncross_val_results = {}\n\n# Perform cross-validation for ridge model\ncross_val_results[\"ridge\"] = pd.DataFrame(\n    cross_validate(ridge_pipeline, X_train, y_train, return_train_score=True)\n).agg(['mean', 'std']).round(3).T\n\n# Perform cross-validation for dummy model\ncross_val_results[\"dummy\"] = pd.DataFrame(\n    cross_validate(dummy_pipeline, X_train, y_train, return_train_score=True)\n).agg(['mean', 'std']).round(3).T\n\n\n\n\n\nridge\ndummy\n\n\n\n\nfit_time\n0.019\n0.012\n\n\nscore_time\n0.004\n0.003\n\n\ntest_score\n0.764\n0.304\n\n\ntrain_score\n0.812\n0.289\n\n\n\nFigure 3: Table showing cross validation results of our ridge and dummy models. Please note that “test_score” here actually refers to the validation score and is different from the test score that we will get from our test data later on."
  },
  {
    "objectID": "posts/guide/guide.html#test-data-and-predictions",
    "href": "posts/guide/guide.html#test-data-and-predictions",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "9. Test data and Predictions",
    "text": "9. Test data and Predictions\n\nOnce you are finally satisfied with your model, test it one last time using the previously unseen “test data”\n\nKeep in mind that you can only do this once, and you should not go back and adjust your model to try and get a better test score as it will no longer be truly “unseen”. We call this the “golden rule”.\n\nCongratulations! your model is finally ready to start making predictions.\n\nThese predictions will likely be as accurate as the test score suggests.\n\nNow all that’s left is to communicate your findings and use those predictions to solve your problem.\n\n\nWell done! You are now ready to tackle your problems using Machine Learning.\n\n\nReferences\n\nDaumé III, H. (retrieved 2024). A Course in Machine Learning (CIML). Retrieved from https://ciml.info/\nMueller, A. C., & Guido, S. (2016). Introduction to Machine Learning with Python: A Guide for Data Scientists. O’Reilly Media.\nRussell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach (4th ed.). Pearson.\nTop IDE Index. (retrieved 2025). PYPL Popularity of Programming Languages. Retrieved from https://pypl.github.io/IDE.html"
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html",
    "title": "Cat breed CNN classifier",
    "section": "",
    "text": "This is a Convolutional Neural Network (CNN) that classifies cats based on their images into one of 6 breeds: 1. American Short hair\n2. Bengal\n3. Maine Soon\n4. Ragdoll\n5. Scottish Fold\n6. Sphinx\nWe will leverage PyTorch for this task."
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#preamble-the-dataset",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#preamble-the-dataset",
    "title": "Cat breed CNN classifier",
    "section": "Preamble: The dataset",
    "text": "Preamble: The dataset\nWe will be using the following public Kaggle dataset: https://www.kaggle.com/datasets/solothok/cat-breed It contains training data which has 200 images for each class and test data that has 50 images for each class."
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#imports",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#imports",
    "title": "Cat breed CNN classifier",
    "section": "Imports",
    "text": "Imports\n\n\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\nimport torch\nfrom torch import nn, optim\nfrom torchvision import datasets, transforms, utils, models\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfrom PIL import Image\n\nplt.rcParams.update({'axes.grid': False})\n\nCNNs are comupationally demanding to run. Ideally, we want to utilize a GPU to improve our performance instead of a CPU. The following code snippet checks that our GPU and Pytorch setup is working.\n\ntorch.cuda.is_available()\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Use GPU (CUDA) if available, else use CPU\nprint(f\"Using device: {device.type}\")"
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#cnn-from-scratch",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#cnn-from-scratch",
    "title": "Cat breed CNN classifier",
    "section": "1. CNN from scratch",
    "text": "1. CNN from scratch\nFirst, let’s try to make a CNN from scratch for this task, and see how well it performs (spoiler: it won’t be anywhere near as good as pre-trained models). This will also helps us understand how CNNs work and how we can adapt them to different tasks.\n\n1.1 Read the data\nWe simply need to set the paths to our training and test sets. Keep in mind that we are not actually reading the data yet, just learning where it is.\n\nimport torchvision\n\nTRAIN_DIR = \"/kaggle/input/cat-breed/cat-breed/TRAIN\"\nTEST_DIR = \"/kaggle/input/cat-breed/cat-breed/TEST\"\n\n\nprint(f\"Classes: {train_dataset.classes}\")\nprint(f\"Class count: {train_dataset.targets.count(0)}, {train_dataset.targets.count(1)}\")\nprint(f\"Samples:\",len(train_dataset))\nprint(f\"First sample: {train_dataset.samples[0]}\")\n\n\n\n1.2 Transform images\nThe images in our dataset have all kinds of different resolutions and aspect ratios, we must normalize them to a specific shape and size to be able to work with them. We will tranform them into 200x200 images.\n\nIMAGE_SIZE = (200, 200) # This does not depend on the size of the raw images\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\ntest_dataset = torchvision.datasets.ImageFolder(root=TEST_DIR, transform=data_transforms)\n\n\n\n1.3 Create batches\nOur dataset is too large to be loaded all at once, we must instead create loaders to feed the data into our model in batches.\n\nBATCH_SIZE = 64\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,          \n    batch_size=BATCH_SIZE,  \n    shuffle=True,           \n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,          \n    batch_size=BATCH_SIZE,  \n    shuffle=True,          \n)\n\n\n# (Optional) Checking our batches\n\nimgs, targets = next(iter(train_loader))\n\nprint(f\"  Number of batches: {len(train_loader)}\")\nprint(f\"    Image data type: {type(imgs)}\")\nprint(f\"   Image batch size: {imgs.shape}\") \nprint(f\"  Target batch size: {targets.shape}\")\n\n\n# A sample of our training data\n\nsample_batch = next(iter(train_loader))\nplt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\nplt.imshow(np.transpose(torchvision.utils.make_grid(sample_batch[0], padding=1, normalize=True),(1,2,0)));\n\n\n\n1.4 Create the CNN\nNow, the fun part. We design a CNN using various functions from the Pytorch package. There is no right or wrong here, we simply experiment with different configuration until we reach a suitabel model.\n\nimport torch\nimport torch.nn as nn\n\nclass cat_CNN(nn.Module):\n    def __init__(self):\n        super(cat_CNN, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Dropout(0.2),\n\n            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Dropout(0.5),\n\n            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Dropout(0.2),\n\n            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Dropout(0.2),\n\n            nn.Flatten(),\n            \n            nn.Linear(256 * 12 * 12, 512),\n            nn.ReLU(),\n\n            nn.Linear(512, 128),\n            nn.ReLU(),\n\n            nn.Linear(128, 6)\n        )\n        \n    def forward(self, x):\n        return self.model(x)\n\n\n\n1.5 Train the model\nIt is time to train the model. In the function below, not only do we train the model, we also evaluate it after every epoch. This is useful for continuously observing the performance of our model, and can help us decide if we need to change the number of epochs, or adjust certain aspects of our model that may be causing overfitting, etc.\nNote: Training CNNs is computationally demanding. I am using Kaggle’s GPU T4 x 2 accelerator to improve performance.\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n    \n    train_loss, valid_loss, valid_accuracy = [], [], []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n        \n        # Training\n        model.train()\n        for X, y in trainloader:\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = model(X)\n            loss = criterion(y_hat, y)\n            loss.backward()\n            optimizer.step()\n            train_batch_loss += loss.item()\n        train_loss.append(train_batch_loss / len(trainloader))\n        \n        # Validation\n        model.eval()\n        \n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood\n            for X, y in validloader:\n                X, y = X.to(device), y.to(device)\n                y_hat = model(X)\n                _, y_hat_labels = torch.softmax(y_hat, dim=1).topk(1, dim=1)\n                loss = criterion(y_hat, y)\n                valid_batch_loss += loss.item()\n                valid_batch_acc += (y_hat_labels.squeeze() == y).type(torch.float32).mean().item()\n        valid_loss.append(valid_batch_loss / len(validloader))\n        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n        \n\n        \n        # Print progress\n        if verbose:\n            print(f\"Epoch {epoch + 1}:\",\n                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n    \n    results = {\"train_loss\": train_loss,\n               \"valid_loss\": valid_loss,\n               \"valid_accuracy\": valid_accuracy}\n    return results    \n\n\n# Training the model and observing its results\n\ncat_model = cat_CNN().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cat_model.parameters(), lr=1e-3)\n\ntorch.manual_seed(792)\n\nresults = trainer(cat_model, criterion, optimizer, train_loader, test_loader, epochs=20)\n\nWe can see that our humble from-scratch model is able to correctly classify cat breeds approximately half the time. We can do a lot better by leveraging pre-trained models as we will explore in the next section."
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#pre-trained-models",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#pre-trained-models",
    "title": "Cat breed CNN classifier",
    "section": "2. Pre-trained models",
    "text": "2. Pre-trained models\nWe will explore how we can leverage pre-trained models to improve our model’s performance. We will use DenseNet to extract features from our images, then feed it to our model to finally classify them.\n\n2.1 Pre-trained model as-is\nThe simplest way to use a pre-trained model is to use it as-is, let’s try that and see how it performs then decide whether or not we want to fine-tune it for our use case.\n\ndensenet = models.densenet121(pretrained=True)\n\nfor param in densenet.parameters():  # Freeze parameters so we don't update them (Keep model as-is)\n    param.requires_grad = False\n\ndensenet.classifier\n\n\n# Our classification layer\ncustom_classification_layer = nn.Sequential(\n    nn.Linear(1024, 50),\n    nn.ReLU(),\n    nn.Linear(50, 6)\n)\n\ndensenet.classifier = custom_classification_layer\n\ndensenet.classifier\n\nNote: The DenseNet model is much larger and more complex than our scratch-made model. It is very computationally demanding.\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device.type}\")\n\ndensenet.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(densenet.parameters(), lr=1e-3)\n\n\nresults = trainer(densenet, criterion, optimizer, train_loader, test_loader, epochs=10)\n\nAfter leveraging the pre-trained model, our performance has become very impressive. Our model can now correctly classify cat breeds based on their images 92% of the time.\n\n\n2.2: Fine-tuned pre-trained model\nThe DenseNet model is incredible, but it is forced to make certain compromises to generalize for different tasks. Let’s try to fine-tune it so that it performs better for our specific cat breed classification task.\nThere are plenty of ways to fine-tune a pre-trained model. In our case, we will explore 2 fine-tuning methods:\n1. Fully unfrozen pre-trained model. 2. Partially unfrozen pre-trained model.\n\n2.2.1 Fully unfrozen pre-trained model\nThis fine-tuning method throws away all of the learned parameters from the pre-trained model, and generates new ones by training it on our dataset. Essentially, it uses the architecture of the pre-trained model but throws away all of its previous training.\n\ndensenet_ft_full = models.densenet121(pretrained=True)\nfor param in densenet_ft_full.parameters():\n    # Freeze parameters so we don't update them\n    param.requires_grad = False\n\ndensenet_ft_full.classifier = custom_classification_layer # replace classification layer with ours\n\n\ndensenet_ft_full.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(densenet_ft_full.parameters(), lr=1e-3)\nresults = trainer(densenet_ft_full, criterion, optimizer, train_loader, test_loader, epochs=10)\n\nWe can see that the fully unfrozen pre-trained model performs very similarly to the as-is pre-trained model. This is mostly coincidental. Let’s see if we can do better.\n\n\n2.2.2 Partially unfrozen pre-trained model\nThis fine-tuning method throws keeps almost everything from the pre-trained model, but fine-tunes a few of its layers (usually the last) to work better for our use case. These models should take less time to train as we’re only training a few layers rather than training all of them.\n\ndensenet_ft_partial = models.densenet121(pretrained=True)\nfor layer in densenet_ft_partial.features[:-1]: # Freezing all but last layer\n    for param in layer.parameters():\n        param.requires_grad = False\n\ndensenet_ft_partial.classifier = custom_classification_layer\n\n\n# Train the model\ndensenet_ft_partial.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(densenet_ft_partial.parameters(), lr=1e-3)\nresults = trainer(densenet_ft_partial, criterion, optimizer, train_loader, test_loader, epochs=10)\n\n\n\n\nRecap:\n\nFrom-scratch model performance: ~0.54 validation accuracy.\nDensenet as-is model performance: ~0.92 validation accuracy.\nDensenet fully unfrozen model performance: ~0.92 validation accuracy.\nDensenet partially frozen model performance: ~0.93 validation accuracy.\n\nAlthough additional fine-tuning is possible, we will likely get diminishing returns.\nIt is very interesting that the partially frozen model performed better than the fully unfrozen model. This suggests that the default weights of the densenet model are better than the weights we are creating as a result of training on our dataset. This makes sense because those default weights were likely created through much more training on much larger datasets. It is impressive that those default weights perform so well on our specific case even though they weren’t created with classifying cat breeds in mind (it may have been part of its training, but definitely not the entire focus)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is where I will share the lessons I learn throughout my ongoing Machine Learning journey."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thamer's ML Blog",
    "section": "",
    "text": "Cat breed CNN classifier\n\n\n\n\n\n\nML Models\n\n\nGuides\n\n\nCode\n\n\nPyTorch\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nThamer Aldawood\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Models: Decision Trees\n\n\n\n\n\n\nML Models\n\n\nGuides\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nFeb 22, 2025\n\n\nThamer Aldawood\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Machine Learning - Beginner’s Guide\n\n\n\n\n\n\nGuides\n\n\nCode\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nThamer Aldawood\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html",
    "href": "posts/decision_trees/decision_trees.html",
    "title": "Machine Learning Models: Decision Trees",
    "section": "",
    "text": "On this page, we explore Decision Trees and showcase one of their use cases."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#imports",
    "href": "posts/decision_trees/decision_trees.html#imports",
    "title": "Machine Learning Models: Decision Trees",
    "section": "Imports",
    "text": "Imports\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score, cross_validate, train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nWe’ll be using Kaggle’s Spotify Song Attributes dataset. The dataset contains a number of features of songs from 2017 and a binary variable target that represents whether the user liked the song (encoded as 1) or not (encoded as 0). See the documentation of all the features here."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#read-and-split-the-dataset",
    "href": "posts/decision_trees/decision_trees.html#read-and-split-the-dataset",
    "title": "Machine Learning Models: Decision Trees",
    "section": "1. Read and split the dataset",
    "text": "1. Read and split the dataset\n\n\nWe use read_csv from the pandas package to read the data.\n\nWe use train_test_split from sklearn to split the data into separate training and test sets. (Not to be confused with validation sets which will be created later from the training set).\n\nThe test_size parameter determines the proportion of the test set to the training set. Generally, a larger training set results in a better model, a larger test set results in a more accurate assessment of the model. We must find a balance between these two.\nNote that the dataset is sorted on the target. If we maintain this list sorting our model will simply predict the target based on the song’s position in the sorted list, rather than its features. This will not help us make predictions for future unseen data. Therefore, we set the first column as the index so that our model does not learn the sorted order of our data.\n\n\n\nspotify_df = pd.read_csv(\"data/spotify.csv\", index_col=0)\n\nspotify_df.head() # to show a sample from the dataset\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nduration_ms\nenergy\ninstrumentalness\nkey\nliveness\nloudness\nmode\nspeechiness\ntempo\ntime_signature\nvalence\ntarget\nsong_title\nartist\n\n\n\n\n0\n0.0102\n0.833\n204600\n0.434\n0.021900\n2\n0.1650\n-8.795\n1\n0.4310\n150.062\n4.0\n0.286\n1\nMask Off\nFuture\n\n\n1\n0.1990\n0.743\n326933\n0.359\n0.006110\n1\n0.1370\n-10.401\n1\n0.0794\n160.083\n4.0\n0.588\n1\nRedbone\nChildish Gambino\n\n\n2\n0.0344\n0.838\n185707\n0.412\n0.000234\n2\n0.1590\n-7.148\n1\n0.2890\n75.044\n4.0\n0.173\n1\nXanny Family\nFuture\n\n\n3\n0.6040\n0.494\n199413\n0.338\n0.510000\n5\n0.0922\n-15.236\n1\n0.0261\n86.468\n4.0\n0.230\n1\nMaster Of None\nBeach House\n\n\n4\n0.1800\n0.678\n392893\n0.561\n0.512000\n5\n0.4390\n-11.648\n0\n0.0694\n174.004\n4.0\n0.904\n1\nParallel Lines\nJunior Boys\n\n\n\n\n\n\n\n\ntrain_df = None\ntest_df = None\n\ntrain_df, test_df = train_test_split(\n    spotify_df, test_size=0.2, random_state=123\n)"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#exploratory-data-analysis-eda",
    "href": "posts/decision_trees/decision_trees.html#exploratory-data-analysis-eda",
    "title": "Machine Learning Models: Decision Trees",
    "section": "2. Exploratory Data Analysis (EDA)",
    "text": "2. Exploratory Data Analysis (EDA)\nIn this section, we want to take a closer look at the dataset so that we can make more informed decisions when designing the model later.\n\n\nn_train_samples = train_df.shape[0]\nn_test_samples = test_df.shape[0]\n\nprint(f\"Number of training samples: {n_train_samples}\")\nprint(f\"Number of test samples: {n_test_samples}\")\n\nNumber of training samples: 1613\nNumber of test samples: 404\n\n\n\nspotify_summary = train_df.describe()\nspotify_summary\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nduration_ms\nenergy\ninstrumentalness\nkey\nliveness\nloudness\nmode\nspeechiness\ntempo\ntime_signature\nvalence\ntarget\n\n\n\n\ncount\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n\n\nmean\n0.185627\n0.616745\n247114.827650\n0.681296\n0.136862\n5.383137\n0.189189\n-7.112929\n0.621203\n0.091277\n121.979777\n3.964662\n0.497587\n0.507750\n\n\nstd\n0.259324\n0.163225\n81177.300308\n0.211612\n0.277744\n3.620422\n0.153170\n3.838867\n0.485238\n0.087890\n26.965641\n0.255201\n0.247378\n0.500095\n\n\nmin\n0.000005\n0.122000\n16042.000000\n0.014800\n0.000000\n0.000000\n0.018800\n-33.097000\n0.000000\n0.023100\n47.859000\n1.000000\n0.035900\n0.000000\n\n\n25%\n0.009190\n0.511000\n200105.000000\n0.564000\n0.000000\n2.000000\n0.092300\n-8.388000\n0.000000\n0.037300\n100.518000\n4.000000\n0.295000\n0.000000\n\n\n50%\n0.062500\n0.629000\n230200.000000\n0.714000\n0.000071\n6.000000\n0.127000\n-6.248000\n1.000000\n0.054900\n121.990000\n4.000000\n0.496000\n1.000000\n\n\n75%\n0.251000\n0.738000\n272533.000000\n0.844000\n0.057300\n9.000000\n0.243000\n-4.791000\n1.000000\n0.107000\n137.932000\n4.000000\n0.690000\n1.000000\n\n\nmax\n0.995000\n0.984000\n849960.000000\n0.997000\n0.976000\n11.000000\n0.969000\n-0.307000\n1.000000\n0.816000\n219.331000\n5.000000\n0.992000\n1.000000\n\n\n\n\n\n\n\nIn the following plots, we explore different features and analyze their relationship with our target. 1 means the user liked the song, 0 means they did not.\n\n# Histogram for loudness\nfeat = \"loudness\"\ntrain_df.groupby(\"target\")[feat].plot.hist(bins=50, alpha=0.5, legend=True, density = True, title = \"Histogram of \" + feat)\nplt.xlabel(feat)\n\nText(0.5, 0, 'loudness')\n\n\n\n\n\n\n\n\n\n\nfor feat in ['acousticness', 'danceability', 'tempo', 'energy', 'valence']: # This loop creates a histogram for each of the features in the list\n    train_df.groupby(\"target\")[feat].plot.hist(bins=50, alpha=0.5, legend=True, density = True, title = \"Histogram of \" + feat)\n    plt.xlabel(feat)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep in mind that even if we see a feature with a histogram that has not discernable patterns with the target, it does not necessarily mean that the feature is not useful for predicting the target. As some patterns only appear when a feature is combined with another. For example: Valence on its own seems insignificant for predicting the target, but that can change when we look at Valence alongside Tempo.\nNote that the dataset includes two text features labeled song_title and artist. For now, we will simply drop these text features as encoding text can be tricky and may derail us from our original goal here, which is to explore decision trees."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#select-features",
    "href": "posts/decision_trees/decision_trees.html#select-features",
    "title": "Machine Learning Models: Decision Trees",
    "section": "3. Select features",
    "text": "3. Select features\n\nIn this section, we select the features we want our model to learn. In our case, we will take all the available features except for song_title and artist. Note that we also need to split our x and y (features and target respectively).\n\nX_train = train_df.drop(columns=['target', 'song_title', 'artist'])\ny_train = train_df['target']\nX_test = test_df.drop(columns=['target', 'song_title', 'artist'])\ny_test = test_df['target']"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#create-and-assess-the-baseline",
    "href": "posts/decision_trees/decision_trees.html#create-and-assess-the-baseline",
    "title": "Machine Learning Models: Decision Trees",
    "section": "4. Create and assess the baseline",
    "text": "4. Create and assess the baseline\n\nIn this section, we create a very simple baseline model which we will use to measure our decision tree model against. In our case, the DummyClassifier will simply predict the most frequent case. Meaning if most songs in our dataset were liked, it will predict that they were all liked.\nWe then use cross_val_score to assess our baseline model.\n\ndum = DummyClassifier(random_state=123, strategy='most_frequent')\ndummy_score = np.mean(cross_val_score(dum, X_train, y_train, cv=10))\ndummy_score\n\n0.5077524729698643"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#create-and-assess-the-decision-tree-model",
    "href": "posts/decision_trees/decision_trees.html#create-and-assess-the-decision-tree-model",
    "title": "Machine Learning Models: Decision Trees",
    "section": "5. Create and assess the Decision Tree model",
    "text": "5. Create and assess the Decision Tree model\n\nIn this section, we finally create the decision tree model, and we assess it using cross_validate. Note that this function fits the model to the dataset as its first step so we don’t need to fit our model beforehand.\n\nspotify_tree = DecisionTreeClassifier(random_state=123)\n\n\ndt_scores_df = pd.DataFrame(cross_validate(spotify_tree, X_train, y_train, cv=10, return_train_score=True))\ndt_scores_df\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_score\ntrain_score\n\n\n\n\n0\n0.010003\n0.001002\n0.697531\n0.999311\n\n\n1\n0.010997\n0.000000\n0.660494\n1.000000\n\n\n2\n0.010998\n0.000000\n0.685185\n0.999311\n\n\n3\n0.010997\n0.001002\n0.639752\n1.000000\n\n\n4\n0.009972\n0.001006\n0.639752\n0.999311\n\n\n5\n0.009011\n0.001000\n0.658385\n0.999311\n\n\n6\n0.009998\n0.001002\n0.639752\n0.999311\n\n\n7\n0.010002\n0.001000\n0.608696\n0.999311\n\n\n8\n0.010505\n0.001003\n0.701863\n0.999311\n\n\n9\n0.010000\n0.001001\n0.695652\n0.999311\n\n\n\n\n\n\n\nThe main number we want to look at here is test_score. We ran 10 different tests on our model, let’s take their mean value and compare it to our baseline.\n\nround(dt_scores_df['test_score'].mean(), 3)\n\n0.663"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#optional-visualize-the-model",
    "href": "posts/decision_trees/decision_trees.html#optional-visualize-the-model",
    "title": "Machine Learning Models: Decision Trees",
    "section": "6. (Optional) Visualize the model",
    "text": "6. (Optional) Visualize the model\n\nIn this section, we use the tree package to visualize our decision tree model to understand it better\n\nspotify_tree.fit(X_train, y_train) # We must fit (train) the model before we visualize it\n\nfeature_names = X_train.columns.tolist() # feature names \nclass_names = [\"Liked\", \"Disliked\"] # unique class names \n\ntoy_tree_viz = tree.plot_tree(spotify_tree, feature_names=feature_names, class_names=class_names, max_depth=1)\n# The tree is too big and complicated to fully visualize, so we set max_depth=2 to visualize the first layers only"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#hyperparameter-optimization",
    "href": "posts/decision_trees/decision_trees.html#hyperparameter-optimization",
    "title": "Machine Learning Models: Decision Trees",
    "section": "6. Hyperparameter optimization",
    "text": "6. Hyperparameter optimization\n\nSo far, we have used the decision tree model in its default configuration and got some decent results. But how can we make it perform better? We need to optimize its hyperparameters. In our case, the decision tree model has a single hyperparameter depth which determines the depths of the decision tree.\nLet’s try out a number of different depths and see which one preforms best.\n\ndepths = np.arange(1, 25, 2)\ndepths\n\narray([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23])\n\n\n\nresults_dict = {\n    \"depth\": [],\n    \"mean_train_score\": [],\n    \"mean_cv_score\": [],\n}\n\nfor depth in depths: # Create a model for each depth in our list, assess it, and add it to our results_df\n    model = DecisionTreeClassifier(max_depth=depth, random_state=123)\n    scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n    results_dict[\"depth\"].append(depth)\n    results_dict[\"mean_cv_score\"].append(np.mean(scores[\"test_score\"]))\n    results_dict[\"mean_train_score\"].append(np.mean(scores[\"train_score\"]))\n\nresults_df = pd.DataFrame(results_dict)\nresults_df = results_df.set_index(\"depth\")\nresults_df\n\n\n\n\n\n\n\n\nmean_train_score\nmean_cv_score\n\n\ndepth\n\n\n\n\n\n\n1\n0.651030\n0.646032\n\n\n3\n0.733485\n0.692524\n\n\n5\n0.794035\n0.711713\n\n\n7\n0.858718\n0.703060\n\n\n9\n0.912930\n0.690610\n\n\n11\n0.955157\n0.680048\n\n\n13\n0.980850\n0.674457\n\n\n15\n0.993525\n0.658979\n\n\n17\n0.998278\n0.669538\n\n\n19\n0.999173\n0.665812\n\n\n21\n0.999449\n0.662706\n\n\n23\n0.999449\n0.662706\n\n\n\n\n\n\n\nWe can see that in our case, depth 5 yields the best result: 0.711713. However, we must also consider the fundamental tradeoff. We want our model to have the highest test scores, but if its training score is too high it may suggest that it is overfitting on our particular dataset and will generalize poorly to future unseen data. To take a closer look at this, let’s plot our model’s scores and see how they change as depth changes.\n\nresults_df[[\"mean_train_score\", \"mean_cv_score\"]].plot()\n\n\n\n\n\n\n\n\n\n\nWe can see that the mean_cv_score peaks at depth 5 then begins to decrease. Whereas the mean_train_score continuously increases. We can conclude that depth 5 is the ideal depth for our model in this use case. This is what we call “The sweet spot”."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#final-model-and-test",
    "href": "posts/decision_trees/decision_trees.html#final-model-and-test",
    "title": "Machine Learning Models: Decision Trees",
    "section": "7. Final model and test",
    "text": "7. Final model and test\n\nIn this section, we recreate our decision tree model using the optimized hyperparameter, then we test it and compare our results with out unoptimized and baseline models.\n\nbest_model = DecisionTreeClassifier(max_depth=5, random_state=123)\nbest_model.fit(X_test, y_test)\ntest_score = best_model.score(X_test, y_test)\ntest_score\n\n0.8267326732673267\n\n\nTo recap: - Baseline model score: ~0.51\n- Unoptimized decision tree model score: ~0.67\n- Optimized decision tree model score: ~0.83"
  }
]