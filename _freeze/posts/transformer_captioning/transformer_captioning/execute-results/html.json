{
  "hash": "52e040454e1d83b238d73c2a594f56f2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Image captioning using a transformer\"\nauthor: \"Thamer Aldawood\"\ndate: \"2025-05-18\"\ncategories: [PyTorch, NLP, LLM, Transformers]\nimage: \"https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/caption-1.png\"\ntoc: true\ntoc-depth: 3\nexecute:\n  eval: false\n---\n\n\n\n\nThis post covers image captioning with a Transformer model, including image feature extraction, tokenization, positional encoding, model training, and generation.\n\n![Captioning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/caption-1.png \"Captioning\")  \n[source](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/caption-1.png)\n\n## Image captioning using a transformer\n<hr>\n\n**Caption generation** is the automated process of creating precise, grammatically sound descriptions for images. It requires recognizing essential objects, actions, and contextual elements within an image, then articulating them in natural language. This task bridges the fields of computer vision and natural language processing, leveraging a vision model to analyze image features and a language model to craft meaningful descriptive text.\n\nThe diagram above provides a high-level overview: a pre-trained computer vision model (such as a CNN) first extracts image features, which are then fed into a language model to generate captions sequentially, word by word.\n\n## Imports\n\n::: {#1cd5c5a2 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport math\nfrom PIL import Image as PIL_Image\nfrom tqdm import tqdm, trange\n```\n:::\n\n\n**Tools we will use:**\n\n- [PyTorch](https://pytorch.org/) \n- [Hugging Face Transformers](https://huggingface.co/)\n- [Pre-trained CNN models](https://huggingface.co/docs/transformers/main/en/model_doc/resnet#transformers.ResNetModel)\n\nThe image captioning task consists of several key components, which we will divide into six distinct tasks:\n- **TASK 1:** Extract image features using pre-trained vision models.\n- **TASK 2:** Tokenize captions using pre-trained language models.\n- **TASK 3:** Create a PyTorch dataset for training.\n- **TASK 4:** Implement the `ImageCaptioning` class, which uses the transformer architecture for caption generation.\n- **TASK 5:** Write the PyTorch training loop to train the model.\n- **TASK 6:** Generate captions and evaluate the generated captions.\n\nWe will train the model using [the flickr8k dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k). The training task will be computationally expensive. We may opt to use Kaggle notebooks, which offer free access to strong GPUs for up to 30 hours per week.\n\n::: {#0716925a .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:00:59.205216Z\",\"iopub.status.busy\":\"2025-04-20T19:00:59.204992Z\",\"iopub.status.idle\":\"2025-04-20T19:00:59.213520Z\",\"shell.execute_reply\":\"2025-04-20T19:00:59.212824Z\",\"shell.execute_reply.started\":\"2025-04-20T19:00:59.205199Z\"}' execution_count=2}\n``` {.python .cell-code}\n# The folder containing the data \n# image_folder = \"flickr8k\"\n\n# If you run the notebook on Kaggle, you can use the following line\nimage_folder = \"/kaggle/input/flickr8k\"\n```\n:::\n\n\n::: {#3f539707 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:00:59.214420Z\",\"iopub.status.busy\":\"2025-04-20T19:00:59.214221Z\",\"iopub.status.idle\":\"2025-04-20T19:00:59.411527Z\",\"shell.execute_reply\":\"2025-04-20T19:00:59.410879Z\",\"shell.execute_reply.started\":\"2025-04-20T19:00:59.214403Z\"}' execution_count=3}\n``` {.python .cell-code}\n# Read and show the first few lines of caption.txt \ndf = pd.read_csv(f\"{image_folder}/captions.txt\", sep=',')\ndf.head()\n```\n:::\n\n\n::: {#dc6d815e .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:00:59.412588Z\",\"iopub.status.busy\":\"2025-04-20T19:00:59.412327Z\",\"iopub.status.idle\":\"2025-04-20T19:00:59.467357Z\",\"shell.execute_reply\":\"2025-04-20T19:00:59.466615Z\",\"shell.execute_reply.started\":\"2025-04-20T19:00:59.412567Z\"}' execution_count=4}\n``` {.python .cell-code}\n# Set the appropriate device depending upon your hardware. \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') \nprint(device)\n```\n:::\n\n\n### TASK 1: Extracting image features using pre-trained vision models\nThe initial step in the image captioning process involves extracting image features. This is commonly achieved by extracting the output from the final linear layer of a Convolutional Neural Network (CNN). Instead of building our own CNN from scratch, we will use a pre-trained CNN model.\n\n::: {#e064d5ae .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:00:59.468316Z\",\"iopub.status.busy\":\"2025-04-20T19:00:59.468095Z\",\"iopub.status.idle\":\"2025-04-20T19:01:19.759572Z\",\"shell.execute_reply\":\"2025-04-20T19:01:19.758809Z\",\"shell.execute_reply.started\":\"2025-04-20T19:00:59.468301Z\"}' execution_count=5}\n``` {.python .cell-code}\nimport torch\nfrom transformers import AutoFeatureExtractor, ResNetModel\n\nclass ImageFeatureExtractor:\n    \"\"\"Extracts image features using a pretrained ResNet model.\"\"\"\n\n    def __init__(self, device, model_name=\"microsoft/resnet-18\"):\n        self.device = device\n        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n        self.resnet_model = ResNetModel.from_pretrained(model_name).to(device)\n        self.resnet_model.eval()\n\n    def __call__(self, image) -> torch.Tensor:\n        \"\"\"Returns a (1, 512) tensor of features extracted from the input image.\"\"\"\n        \n        inputs = self.feature_extractor(images=image, return_tensors=\"pt\").to(self.device)\n\n        # Extract features using the ResNet model\n        with torch.no_grad():  # Disable gradient calculation\n            outputs = self.resnet_model(**inputs)\n\n        # Retrieve the features after the final pooling layer\n        features = outputs.pooler_output\n\n        # Ensure the feature shape is (1, 512)\n        features = features.view(1, -1)\n\n        # Return the features, moved back to CPU for processing compatibility\n        return features.cpu()\n```\n:::\n\n\n::: {#6994b941 .cell execution='{\"execution_failed\":\"2025-04-20T21:04:10.406Z\",\"iopub.execute_input\":\"2025-04-20T19:01:19.760825Z\",\"iopub.status.busy\":\"2025-04-20T19:01:19.760387Z\"}' execution_count=6}\n``` {.python .cell-code}\n# Now let's open an image \nimage = PIL_Image.open('{}/Images/{}'.format(image_folder, df.iloc[34396].image))\nimage\n```\n:::\n\n\n![image.png](attachment:image.png)\n\n::: {#c2b9aea0 .cell execution_count=7}\n``` {.python .cell-code}\nfeature_extractor = ImageFeatureExtractor(device=device) # We'll need this later\n```\n:::\n\n\n### TASK 2: Implement a tokenizer using pre-trained language models\n\nNext, we'll preprocess text data by tokenizing it using Hugging Face's [AutoTokenizer](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#autotokenizer), specifically the `bert-base-cased` model. Since the vocabulary of `bert-base-cased` is significantly larger than that of our captions dataset, we'll create a wrapper class for `AutoTokenizer` to define custom token-to-vocabulary mappings.\n\n::: {#c5b2f001 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:21.645871Z\",\"iopub.status.busy\":\"2025-04-20T19:01:21.645590Z\",\"iopub.status.idle\":\"2025-04-20T19:01:21.673662Z\",\"shell.execute_reply\":\"2025-04-20T19:01:21.673088Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:21.645848Z\"}' execution_count=8}\n``` {.python .cell-code}\nfrom transformers import AutoTokenizer\nfrom tqdm import trange\n\nclass TokenizerWrapper:\n    \"\"\"Wraps AutoTokenizer with a custom vocabulary mapping.\"\"\"\n\n    def __init__(self, model_name=\"bert-base-cased\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        # Initialize mappings with special tokens: [PAD] -> 0, [CLS] -> 1, [SEP] -> 2\n        self.token_id_to_vocab_id = {0: 0, 101: 1, 102: 2}\n        self.vocab_id_to_token_id = {0: 0, 1: 101, 2: 102}\n        \n        self.vocab_id = 3  # Start after special tokens\n        self.padding_len = None\n\n    def build_dictionary(self, captions: list[str]):\n        \"\"\"Builds vocabulary from a list of captions and sets padding length.\"\"\"\n        tokenized = self.tokenizer(captions, padding='longest').input_ids\n        self.padding_len = len(tokenized[0])\n\n        for tokens in tokenized:\n            for token_id in tokens:\n                if token_id not in self.token_id_to_vocab_id:\n                    self.token_id_to_vocab_id[token_id] = self.vocab_id\n                    self.vocab_id_to_token_id[self.vocab_id] = token_id\n                    self.vocab_id += 1\n\n    def get_vocab_size(self) -> int:\n        \"\"\"Returns the size of the custom vocabulary.\"\"\"\n        assert len(self.token_id_to_vocab_id) == len(self.vocab_id_to_token_id)\n        return self.vocab_id\n\n    def tokenize(self, text: str) -> list[int]:\n        \"\"\"Tokenizes text using custom vocabulary (requires build_dictionary first).\"\"\"\n        assert self.padding_len is not None, \"Call build_dictionary() before tokenizing.\"\n        token_ids = self.tokenizer(text, padding='max_length', max_length=self.padding_len).input_ids\n        return [self.token_id_to_vocab_id[token_id] for token_id in token_ids]\n\n    def decode(self, vocab_ids: list[int]) -> str:\n        \"\"\"Decodes a list of custom vocab IDs into a string.\"\"\"\n        token_ids = [self.vocab_id_to_token_id[vocab_id] for vocab_id in vocab_ids]\n        # Using `self.tokenizer.decode` to convert a list of token IDs back into a text string.\n        text = self.tokenizer.decode(token_ids)\n        return text.replace('[CLS] ', '').replace(' [SEP]', '').replace(' [PAD]', '')\n```\n:::\n\n\n::: {#45325b89 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:21.677818Z\",\"iopub.status.busy\":\"2025-04-20T19:01:21.677622Z\",\"iopub.status.idle\":\"2025-04-20T19:01:25.347595Z\",\"shell.execute_reply\":\"2025-04-20T19:01:25.346976Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:21.677804Z\"}' execution_count=9}\n``` {.python .cell-code}\n# Build the dictionary for our tokenizer  \ntokenizer_wrapper = TokenizerWrapper()\ntokenizer_wrapper.build_dictionary(df[\"caption\"].to_list())\n```\n:::\n\n\n::: {#289fa8b8 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:25.348433Z\",\"iopub.status.busy\":\"2025-04-20T19:01:25.348251Z\",\"iopub.status.idle\":\"2025-04-20T19:01:25.356368Z\",\"shell.execute_reply\":\"2025-04-20T19:01:25.355562Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:25.348418Z\"}' execution_count=10}\n``` {.python .cell-code}\n# What's the size of our custom vocabulary\ntokenizer_wrapper.get_vocab_size() \n```\n:::\n\n\n::: {#c822b9bb .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:25.357408Z\",\"iopub.status.busy\":\"2025-04-20T19:01:25.357115Z\",\"iopub.status.idle\":\"2025-04-20T19:01:25.428946Z\",\"shell.execute_reply\":\"2025-04-20T19:01:25.428355Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:25.357385Z\"}' execution_count=11}\n``` {.python .cell-code}\ntokenizer_wrapper.tokenizer.vocab_size\n```\n:::\n\n\n::: {#52853cd5 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:25.430380Z\",\"iopub.status.busy\":\"2025-04-20T19:01:25.429815Z\",\"iopub.status.idle\":\"2025-04-20T19:01:25.444443Z\",\"shell.execute_reply\":\"2025-04-20T19:01:25.443733Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:25.430357Z\"}' execution_count=12}\n``` {.python .cell-code}\n# let's try to tokenize the caption corresponding to the image we saw in the last task, \n# and decode the tokens back to the caption\n\ncaption_tokens = tokenizer_wrapper.tokenize(df.iloc[34396].caption)\ndecoeded_caption = tokenizer_wrapper.decode(caption_tokens)\nprint('Caption:', df.iloc[34396].caption)\nprint('Tokens:', caption_tokens)\nprint('Decoded caption:', decoeded_caption)\n\n# Our Caption and the Decoded caption should match here. \n```\n:::\n\n\n### TASK 3: Data splitting and creating a Pytorch dataset\n#### Part 1: Data splitting\n\nUp to this point, we've developed an image feature extractor to generate feature vectors from images and a tokenizer to encode captions into meaningful representations. Now, it's time to prepare our data for model training and evaluation.  \n\nLet's start by dividing the dataset into training and testing subsets.\n\n::: {#37508460 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:25.445786Z\",\"iopub.status.busy\":\"2025-04-20T19:01:25.445402Z\",\"iopub.status.idle\":\"2025-04-20T19:01:25.462787Z\",\"shell.execute_reply\":\"2025-04-20T19:01:25.462116Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:25.445762Z\"}' execution_count=13}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\ndef train_test_split_by_image(data_df, sample_size=None, train_ratio=0.8, seed=100):\n    \"\"\"\n    Splits the dataframe into training and testing datasets based on unique images.\n    \n    Parameters:\n        data_df (pandas.DataFrame): The dataset to split, containing at least 'image' and 'caption' columns.\n        sample_size (int): The number of samples to consider. Useful during prototyping.\n        train_ratio (float): The proportion of the dataset to allocate to the training set.\n        seed (int): Seed for random number generator for reproducibility.\n\n    Returns:\n        train_df (pandas.DataFrame): Training dataset.\n        test_df (pandas.DataFrame): Testing dataset.\n    \"\"\"\n    np.random.seed(seed)\n    unique_images = np.random.permutation(data_df['image'].unique())\n\n    if sample_size:\n        unique_images = unique_images[:sample_size]\n\n    split_point = int(len(unique_images) * train_ratio)\n    train_images, test_images = unique_images[:split_point], unique_images[split_point:]\n\n    train_df = data_df[data_df['image'].isin(train_images)]\n    test_df = data_df[data_df['image'].isin(test_images)]\n\n    return train_df, test_df\n```\n:::\n\n\n::: {#01e74bb2 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:25.463701Z\",\"iopub.status.busy\":\"2025-04-20T19:01:25.463502Z\",\"iopub.status.idle\":\"2025-04-20T19:01:25.496144Z\",\"shell.execute_reply\":\"2025-04-20T19:01:25.495435Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:25.463686Z\"}' execution_count=14}\n``` {.python .cell-code}\n# Using a sample of the data for prototyping \ntrain_df, test_df = train_test_split_by_image(df, sample_size = 100, train_ratio=0.8)\nprint(f'The number of rows in the training set is {train_df.shape[0]} and the number of unique images is {int(train_df.shape[0]/5)}')\nprint(f'The number of rows in the test set is {test_df.shape[0]} and the number of unique images is {int(test_df.shape[0]/5)}')\ntrain_df.head()\n```\n:::\n\n\n::: {#2b4d64a9 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:25.497063Z\",\"iopub.status.busy\":\"2025-04-20T19:01:25.496886Z\",\"iopub.status.idle\":\"2025-04-20T19:01:25.513842Z\",\"shell.execute_reply\":\"2025-04-20T19:01:25.513275Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:25.497049Z\"}' execution_count=15}\n``` {.python .cell-code}\n# Using the entire dataset\ntrain_df, test_df = train_test_split_by_image(df, train_ratio=0.8)\nprint(f'The number of rows in the training set is {train_df.shape[0]} and the number of unique images is {train_df.shape[0]/5}')\nprint(f'The number of rows in the test set is {test_df.shape[0]} and the number of unique images is {test_df.shape[0]/5}')\ntrain_df.head()\n```\n:::\n\n\nNow that we have train and test datasets, the next steps involve processing each image and caption in both the training and testing sets while ensuring efficient batch loading and handling with PyTorch. To accomplish this, we'll first write a function called `build_data` that converts the data into a torch tensor. After that, we'll implement a class named `PytorchDataset` to construct a PyTorch dataset, making data management more streamlined.\n\n#### Part 2: creating a Pytorch dataset\n\n::: {#e5cb7aaa .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:25.514710Z\",\"iopub.status.busy\":\"2025-04-20T19:01:25.514512Z\",\"iopub.status.idle\":\"2025-04-20T19:01:25.520328Z\",\"shell.execute_reply\":\"2025-04-20T19:01:25.519643Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:25.514695Z\"}' execution_count=16}\n``` {.python .cell-code}\ndef build_data(data_df, tokenizer_wrapper, feature_extractor, image_folder):\n    \"\"\"\n    Constructs a dataset list from provided image and caption data.\n\n    This function processes each entry in a dataframe, extracts image features using a specified\n    feature extractor, tokenizes captions using a tokenizer wrapper, and combines these elements into\n    a list where each item is a dictionary containing image features and tokenized caption data.\n\n    Parameters:\n        data_df (pandas.DataFrame): A dataframe containing at least two columns: 'image' and 'caption'.\n            - 'image' column contains filenames of images.\n            - 'caption' column contains the corresponding captions for the images.\n        tokenizer_wrapper (TokenizerWrapper): An object encapsulating a tokenizer that provides\n            a method 'tokenize' to convert text captions into token ids.\n        feature_extractor (Callable): A function or callable object that accepts an image object and\n            returns a tensor representing extracted features from the image.\n        image_folder (str): The path to the folder where images are stored. The path should not\n            end with a slash. It is assumed that images are stored within an 'Images' subfolder.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary has two keys:\n            - 'image': a tensor containing features of the image.\n            - 'token': a tensor of token ids derived from the caption.\n\n    Example:\n        Assuming 'data_df' has two columns 'image' and 'caption' where 'image' contains filenames:\n        >>> data_df = pd.DataFrame({\n        ...     'image': ['image1.jpg', 'image2.jpg'],\n        ...     'caption': ['A cat on a mat.', 'A dog in the fog.']\n        ... })\n        >>> dataset = build_data(data_df, tokenizer_wrapper, feature_extractor, '/path/to/images')\n        >>> print(dataset[0]['image'])  # Outputs the tensor of image features for 'image1.jpg'\n        >>> print(dataset[0]['token'])  # Outputs the tensor of token ids for 'A cat on a mat.'\n    \"\"\"    \n\n    dataset = []\n    for i in trange(len(data_df)):\n        row = data_df.iloc[i]\n        image_path = f\"{image_folder}/Images/{row.image}\"\n        image = PIL_Image.open(image_path)\n\n        # Get image features \n        image_features = feature_extractor(image)\n\n        # Get caption tokens\n        caption_tokens = torch.tensor(tokenizer_wrapper.tokenize(row.caption))\n        \n        dataset.append({'image': image_features, 'token': caption_tokens})\n    return dataset\n\n```\n:::\n\n\nLet's create train and test datasets by calling `build_data` on train and test splits. \nNote: This will take some time to run, as we are processing all images and captions in the dataset. \n\n::: {#655441ac .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:01:25.521279Z\",\"iopub.status.busy\":\"2025-04-20T19:01:25.520986Z\",\"iopub.status.idle\":\"2025-04-20T19:11:00.934170Z\",\"shell.execute_reply\":\"2025-04-20T19:11:00.933511Z\",\"shell.execute_reply.started\":\"2025-04-20T19:01:25.521257Z\"}' execution_count=17}\n``` {.python .cell-code}\ntrain_data = build_data(train_df, tokenizer_wrapper, feature_extractor, image_folder)\ntest_data = build_data(test_df, tokenizer_wrapper, feature_extractor, image_folder)\n```\n:::\n\n\n::: {#31245946 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:00.935243Z\",\"iopub.status.busy\":\"2025-04-20T19:11:00.935019Z\",\"iopub.status.idle\":\"2025-04-20T19:11:00.939747Z\",\"shell.execute_reply\":\"2025-04-20T19:11:00.938861Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:00.935225Z\"}' execution_count=18}\n``` {.python .cell-code}\n# Get the dimension of the image feature\nimage_dim = train_data[0]['image'].shape[-1]\nvocab_size = tokenizer_wrapper.get_vocab_size()\nprint(f'Image feature dimension is {image_dim}. The vocab size is {vocab_size}.')\n\n# our image_dim should pass this test\nassert isinstance(image_dim, int)\n```\n:::\n\n\nNow let's buid our PyTorch dataset. \n\n::: {#0c799e47 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:00.940658Z\",\"iopub.status.busy\":\"2025-04-20T19:11:00.940417Z\",\"iopub.status.idle\":\"2025-04-20T19:11:00.957606Z\",\"shell.execute_reply\":\"2025-04-20T19:11:00.956839Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:00.940638Z\"}' execution_count=19}\n``` {.python .cell-code}\nclass PytorchDataset:\n    \"\"\"\n    A PyTorch-compatible dataset that returns:\n    - the original token sequence,\n    - image features,\n    - and the target sequence (input shifted left with padding at the end).\n    \"\"\"\n\n    def __init__(self, data, pad_vocab_id=0):\n        self.data = data\n        self.pad_token = torch.tensor([pad_vocab_id])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        tokens = self.data[idx]['token']\n        image = self.data[idx]['image']\n        target = torch.cat([tokens[1:], self.pad_token])\n        return tokens, image, target\n```\n:::\n\n\n::: {#4cb73d4c .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:00.958555Z\",\"iopub.status.busy\":\"2025-04-20T19:11:00.958349Z\",\"iopub.status.idle\":\"2025-04-20T19:11:00.976059Z\",\"shell.execute_reply\":\"2025-04-20T19:11:00.975424Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:00.958532Z\"}' execution_count=20}\n``` {.python .cell-code}\ntrain_dataset = PytorchDataset(train_data)\ntest_dataset = PytorchDataset(test_data)\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n```\n:::\n\n\n::: {#a0d33b30 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:00.976937Z\",\"iopub.status.busy\":\"2025-04-20T19:11:00.976737Z\",\"iopub.status.idle\":\"2025-04-20T19:11:00.989978Z\",\"shell.execute_reply\":\"2025-04-20T19:11:00.989441Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:00.976921Z\"}' execution_count=21}\n``` {.python .cell-code}\n# Now let's get the first data from PytorchDataset\ntrain_token, train_image, train_target = train_dataset[0]\n\n# our dataset should pass these tests\nassert train_token.shape == train_target.shape\nassert torch.all(train_token[1:] == train_target[:-1]).item()\n```\n:::\n\n\n::: {#8184df02 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:00.990758Z\",\"iopub.status.busy\":\"2025-04-20T19:11:00.990561Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.003646Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.002907Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:00.990744Z\"}' execution_count=22}\n``` {.python .cell-code}\n# Shape of the image features\ntrain_image.shape\n```\n:::\n\n\n::: {#b1a1a735 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.004659Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.004463Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.018716Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.017968Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.004638Z\"}' execution_count=23}\n``` {.python .cell-code}\n# Numericalized caption\ntrain_token \n```\n:::\n\n\n::: {#14392635 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.020066Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.019519Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.032870Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.032185Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.020040Z\"}' execution_count=24}\n``` {.python .cell-code}\n# decoded caption\ntokenizer_wrapper.decode(train_token.tolist()) \n```\n:::\n\n\n::: {#65c5950d .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.033890Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.033634Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.046692Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.046118Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.033869Z\"}' execution_count=25}\n``` {.python .cell-code}\n# the target of the train_token (i.e., training example) \ntrain_target\n```\n:::\n\n\n::: {#922234a2 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.047576Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.047379Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.060047Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.059467Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.047562Z\"}' execution_count=26}\n``` {.python .cell-code}\n# decoded caption\ntokenizer_wrapper.decode(train_target.tolist()) \n```\n:::\n\n\nThe decoded caption is the same for input and target because the special token [CLS] with token ID 1 is **not** displayed in the decoded caption. \n\n::: {#e77ad9f1 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.061059Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.060796Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.078427Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.077715Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.061034Z\"}' execution_count=27}\n``` {.python .cell-code}\n# Now let's get a batch of data from DataLoader\ntrain_text, train_image, train_target = next(iter(train_dataloader))\ntrain_text = train_text.to(device)\ntrain_image = train_image.to(device)\n\nassert train_image.ndim == 3\nassert train_text.ndim == 2\n```\n:::\n\n\n::: {#8b591ceb .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.079256Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.079043Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.083729Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.083033Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.079241Z\"}' execution_count=28}\n``` {.python .cell-code}\ntrain_text.shape # batch_size, max_len\n```\n:::\n\n\n::: {#7723b54c .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.084597Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.084346Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.099095Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.098505Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.084574Z\"}' execution_count=29}\n``` {.python .cell-code}\ntrain_image.shape\n```\n:::\n\n\n::: {#544b0e45 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.100204Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.099922Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.112589Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.112026Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.100181Z\"}' execution_count=30}\n``` {.python .cell-code}\ntrain_target.shape # batch_size, max_len\n```\n:::\n\n\n### TASK 4: Implementing the image captioning model using PyTorch\n<hr>\n\nTo summarize our model's architecture, image features are first projected into the embedding space via self.image_embedding and then reshaped to serve as the decoder's memory. Token IDs undergo embedding through self.text_embedding and are further enriched with positional information using self.pos_encoding. These processed inputs are then fed into the TransformerDecoder, where context-aware token representations are generated. The decoderâ€™s output is passed through linear_layer to compute vocabulary logits, followed by a softmax function to derive token probabilities. During inference, a token is sampled from this distribution and used to generate the subsequent word.\n\nThe `PositionalEncoding` class adds sinusoidal positional encodings to input embeddings, helping the Transformer model understand word order in a sequence. Since Transformers lack inherent sequential processing, this encoding is crucial for generating structured captions in our image captioning task.\n\n::: {#56644b53 .cell execution_count=31}\n``` {.python .cell-code}\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Implements sinusoidal positional encoding as described in \"Attention is All You Need\".\n\n    Args:\n        d_model (int): Dimension of the embedding space.\n        dropout (float): Dropout rate after adding positional encodings.\n        max_len (int): Maximum length of supported input sequences.\n    \"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Create a (max_len, 1) position tensor: [[0], [1], ..., [max_len-1]]\n        positions = torch.arange(max_len).unsqueeze(1)\n\n        # Compute the scaling terms for each dimension (even indices only)\n        scale_factors = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n\n        # Initialize the positional encoding matrix with shape (max_len, 1, d_model)\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(positions * scale_factors)  # Apply sine to even indices\n        pe[:, 0, 1::2] = torch.cos(positions * scale_factors)  # Apply cosine to odd indices\n\n        # Register as buffer (not a trainable parameter)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Adds positional encoding to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, d_model)\n\n        Returns:\n            torch.Tensor: Tensor with positional encoding added.\n        \"\"\"\n        seq_len = x.size(0)\n        x = x + self.pe[:seq_len]\n        return self.dropout(x)\n```\n:::\n\n\nNow we are ready to define our own transformer model tailored for image captioning! \n\n::: {#a4f4f13c .cell editable='true' execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.113615Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.113351Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.126860Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.126076Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.113593Z\"}' slideshow='{\"slide_type\":\"\"}' tags='[]' execution_count=32}\n``` {.python .cell-code}\nclass ImageCaptionModel(nn.Module):\n    def __init__(self, d_model, n_heads, num_layers, image_dim, vocab_size, device, dropout=0.1):\n        \"\"\"\n        Initialize the ImageCaptionModel which uses a transformer decoder architecture\n        for generating image captions.\n\n        Parameters:\n            d_model (int): The number of expected features in the encoder/decoder inputs.\n            n_heads (int): The number of heads in the multiheadattention models.\n            num_layers (int): The number of sub-decoder-layers in the transformer.\n            image_dim (int): The dimensionality of the input image features.\n            vocab_size (int): The size of the vocabulary.\n            device (torch.device): The device on which the model will be trained.\n            dropout (float): The dropout value used in PositionalEncoding and TransformerDecoderLayer.\n        \"\"\"        \n        super(ImageCaptionModel, self).__init__()\n        self.d_model = d_model\n        self.device = device\n        # Positional Encoding to add position information to input embeddings\n        self.pos_encoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n\n        # Here is our transformer architecture\n        \n        # Transformer Decoder to generate captions from image features and text inputs\n        self.TransformerDecoder = nn.TransformerDecoder(\n            decoder_layer=nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout), \n            num_layers=num_layers\n        )\n\n        # Linear layer to convert image features to a dimensionality that matches the model's\n        self.image_embedding = nn.Linear(image_dim , d_model)\n\n        # Embedding layer for converting input text tokens into vectors\n        self.text_embedding = nn.Embedding(vocab_size , d_model)\n\n        # Final linear layer to map the output of the transformer decoder to vocabulary size        \n        self.linear_layer = nn.Linear(d_model, vocab_size)\n\n        # Initialize the weights of the model\n        self.init_weights()\n        \n    def init_weights(self):\n        \"\"\"\n        Initialize weights of the model to small random values.\n        \"\"\"\n        initrange = 0.1\n        self.text_embedding.weight.data.uniform_(-initrange, initrange)\n        self.image_embedding.weight.data.uniform_(-initrange, initrange)\n        self.linear_layer.bias.data.zero_()\n        self.linear_layer.weight.data.uniform_(-initrange, initrange)\n        \n    def forward(self, image, text):\n        \"\"\"\n        Defines the forward pass of the model.\n\n        Parameters:\n            image (Tensor): The tensor containing image features.\n            text (Tensor): The tensor containing input text tokens.\n\n        Returns:\n            Tensor: The output tensor containing the logit scores for each vocabulary token.\n        \"\"\"                \n        # Convert image features to match the dimensionality of the model\n        encoded_image = self.image_embedding(image)\n        encoded_image = encoded_image.permute(1,0,2)\n\n        # Convert text tokens into embeddings and apply positional encoding\n        encoded_text = self.text_embedding(text) * math.sqrt(self.d_model)        \n        encoded_text = encoded_text.permute(1,0,2)        \n        encoded_text = self.pos_encoding(encoded_text)\n        \n        # Get the length of the sequences to be decoeded. This is needed to generate the causal masks\n        seq_len = encoded_text.size(0)  \n\n        # Generate a causal mask to prevent the model from attending to future tokens\n        causal_mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n        causal_mask = causal_mask.float().masked_fill(causal_mask == 0, float('-inf')).masked_fill(causal_mask == 1, float(0.0))\n        causal_mask = causal_mask.to(self.device)\n\n        # Process through the transformer decoder\n        transformer_output = self.TransformerDecoder(tgt=encoded_text, memory=encoded_image, tgt_mask=causal_mask) \n\n        # Apply the final linear layer to produce predictions for each token\n        final_layer_output = self.linear_layer(transformer_output) \n        return final_layer_output\n```\n:::\n\n\n::: {#a5a0ddf5 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.128220Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.127623Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.279392Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.278551Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.128197Z\"}' execution_count=33}\n``` {.python .cell-code}\n# Now let's try the model. \n# Feel free to change these hyperparameters. \nd_model = 256 \nn_heads = 4\nnum_layers = 8\nmodel = ImageCaptionModel(d_model=d_model, n_heads=n_heads, num_layers=num_layers, image_dim=image_dim, vocab_size=vocab_size, device=device).to(device)\n```\n:::\n\n\n::: {#fd1c75a3 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.280508Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.280278Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.631546Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.630956Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.280492Z\"}' execution_count=34}\n``` {.python .cell-code}\n# pass inputs to the model\noutput = model(train_image, train_text)\n\n# output should pass the test\nassert output.shape == (train_text.shape[1], train_text.shape[0], vocab_size)\n```\n:::\n\n\n### TASK 5: Implement the training loop\nThe next step is to train our `ImageCaptionModel` using the dataset we've prepared. We are defining the optimizer and the loss function below. \n\n::: {#e34e62b4 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.632686Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.632436Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.637480Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.636667Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.632668Z\"}' execution_count=35}\n``` {.python .cell-code}\n# Feel free to change the hyperparameters. \n\nnum_epoch = 30\nclip_norm = 1.0\nlr = 5e-5\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=0) # Ignore the padding index\n```\n:::\n\n\n::: {#bbcf3280 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.638425Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.638244Z\",\"iopub.status.idle\":\"2025-04-20T19:11:01.653083Z\",\"shell.execute_reply\":\"2025-04-20T19:11:01.652421Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.638399Z\"}' execution_count=36}\n``` {.python .cell-code}\ndef trainer(model, criterion, optimizer, train_dataloader, test_dataloader, epochs=5, patience=5):\n    \"\"\"\n    Trains and evaluates a model using the specified dataloaders, optimizer, and loss criterion with early stopping.\n    \"\"\"    \n    train_losses = []\n    test_losses = []\n    consec_increases = 0        \n    \n    for epoch in range(epochs):\n        sum_train_loss = 0\n        sum_test_loss = 0\n        num_train_loss = 0        \n        num_test_loss = 0\n\n        # Set model to training mode\n        model.train()\n\n        for train_text, train_image, target_seq in train_dataloader:\n\n            train_text = train_text.to(device)\n            train_image = train_image.to(device)\n            target_seq = target_seq.to(device)\n\n            # Forward pass\n            output = model(train_image, train_text) \n            output = output.permute(1, 2, 0) \n            train_loss = criterion(output, target_seq) \n\n            # Backward pass \n            optimizer.zero_grad()        \n            train_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n            optimizer.step()\n\n            # Book keeping \n            sum_train_loss += torch.sum(train_loss).detach().item()\n            num_train_loss += 1 \n\n        # Set the model to evaluation mode\n        model.eval()\n        with torch.no_grad():\n            for test_text, test_image, target_seq in test_dataloader:\n                test_text = test_text.to(device)\n                test_image = test_image.to(device)\n                target_seq = target_seq.to(device)\n\n                # Forward pass\n                output = model(test_image, test_text) \n                output = output.permute(1, 2, 0) \n                test_loss = criterion(output, target_seq)\n                \n                # Book keeping \n                sum_test_loss += torch.sum(test_loss).detach().item()\n                num_test_loss += 1\n\n        # Calculate and store average losses\n        avg_train_loss = sum_train_loss / num_train_loss\n        avg_test_loss = sum_test_loss / num_test_loss\n        train_losses.append(avg_train_loss)\n        test_losses.append(avg_test_loss)\n        print(f\"Epoch {epoch+1}: Train Loss {avg_train_loss:.2f}, Test Loss {avg_test_loss:.2f}\")\n\n        # Early stopping\n        if epoch > 0 and test_losses[-1] > test_losses[-2]:\n            consec_increases += 1\n        else:\n            consec_increases = 0\n        if consec_increases == patience:\n            print(f\"Stopped early at epoch {epoch + 1} - test loss increased for {consec_increases} consecutive epochs!\")\n            break\n            \n    return train_losses, test_losses\n\n```\n:::\n\n\n::: {#3042c63c .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:11:01.654322Z\",\"iopub.status.busy\":\"2025-04-20T19:11:01.653846Z\",\"iopub.status.idle\":\"2025-04-20T19:30:20.934068Z\",\"shell.execute_reply\":\"2025-04-20T19:30:20.933209Z\",\"shell.execute_reply.started\":\"2025-04-20T19:11:01.654299Z\"}' execution_count=37}\n``` {.python .cell-code}\n# Warning: This may take a while to run.\ntrain_losses, test_losses = trainer(model, criterion, optimizer,train_dataloader, test_dataloader, epochs= num_epoch)\n```\n:::\n\n\n### TASK 6: Generate captions using the trained model\nNow that we have trained our model, the moment of truth has arrived! The final task is generating captions using `ImageCaptionModel`.\n\n::: {#986135a0 .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:30:20.935817Z\",\"iopub.status.busy\":\"2025-04-20T19:30:20.935585Z\",\"iopub.status.idle\":\"2025-04-20T19:30:20.942287Z\",\"shell.execute_reply\":\"2025-04-20T19:30:20.941627Z\",\"shell.execute_reply.started\":\"2025-04-20T19:30:20.935798Z\"}' execution_count=38}\n``` {.python .cell-code}\ndef get_test_data(index): \n    \"\"\"\n    Retrieve the text and image data from the test dataset at the specified index.\n\n    Parameters:\n        index (int): The index of the test dataset from which to fetch the data.\n\n    Returns:\n        tuple: A tuple containing:\n            - test_text (Tensor): The tensor representing the caption.\n            - test_image (Tensor): The tensor representing the image.\n    \"\"\"    \n    test_text, test_image, target_seq = test_dataset[index]\n    return test_text, test_image\n    \ndef generate_caption(model, image, device, max_caption_length=100, start_vocab=1, end_vocab=2):\n    \"\"\"\n    Generates a caption for an image using the specified model and device.\n\n    Parameters:\n        model (torch.nn.Module): The trained model used for generating captions.\n        image (torch.Tensor): The image tensor for which to generate the caption.\n        device (torch.device): The device (e.g., CPU or GPU) to which tensors will be sent for model execution.\n        max_caption_length (int, optional): The maximum length of the generated caption. Defaults to 100.\n        start_vocab (int, optional): The vocabulary index used to signify the start of a caption. Defaults to 1.\n        end_vocab (int, optional): The vocabulary index used to signify the end of a caption. Defaults to 2.\n        \n    Returns:\n        numpy.ndarray: An array containing the sequence of vocabulary indices representing the generated caption.\n        \n    \"\"\"    \n    image = torch.unsqueeze(image, dim=0).to(device)\n    context = torch.tensor([[start_vocab]]).to(device)\n    for _ in range(max_caption_length):\n        logits = model(image, context)[-1]\n        probabilities = torch.softmax(logits, dim=-1).flatten(start_dim=1)\n        next_vocab = torch.multinomial(probabilities, num_samples=1)\n        context = torch.cat([context, next_vocab], dim=1)\n        if next_vocab.item() == end_vocab:\n            break\n    return context.cpu().numpy().flatten()\n```\n:::\n\n\n::: {#2a4c7e6a .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:30:20.943369Z\",\"iopub.status.busy\":\"2025-04-20T19:30:20.943082Z\",\"iopub.status.idle\":\"2025-04-20T19:30:20.966234Z\",\"shell.execute_reply\":\"2025-04-20T19:30:20.965367Z\",\"shell.execute_reply.started\":\"2025-04-20T19:30:20.943346Z\"}' execution_count=39}\n``` {.python .cell-code}\nimport random \n\nrandom.seed(10)\n\ndef display_img_captions(image_folder, n_samples = 5, indices=None):\n    \n    # Set a fixed size for images\n    desired_size = (200, 200)\n\n    # If no specific indices are provided, select random samples\n    if indices == None:\n        indices = random.sample(range(test_df.shape[0]), n_samples)\n        print(indices)\n    \n    for index in indices: \n        test_text, test_image = get_test_data(index=index)\n        generated_vocab = generate_caption(model, test_image, device, max_caption_length=100)\n        generated_caption = tokenizer_wrapper.decode(generated_vocab)\n        gold_caption = tokenizer_wrapper.decode(test_text.numpy())\n\n        # Load and prepare the image\n        image_path = '{}/Images/{}'.format(image_folder, test_df.iloc[index]['image'])\n        image = PIL_Image.open(image_path)\n        img = image.resize(desired_size)\n        \n        # Display the image\n        plt.figure(figsize=(5, 5))  # Create a new figure for each image\n        plt.imshow(img)\n        plt.axis('off')  # Turn off axis numbers and ticks\n        plt.title(f\"Generated: {generated_caption}\\nActual: {gold_caption}\")\n        plt.show()\n        \n        # print('Generated: ', generated_caption)\n        # print('Actual: ', gold_caption)\n        print('\\n\\n')\n```\n:::\n\n\n::: {#987f271a .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:30:20.967419Z\",\"iopub.status.busy\":\"2025-04-20T19:30:20.967100Z\",\"iopub.status.idle\":\"2025-04-20T19:30:20.986329Z\",\"shell.execute_reply\":\"2025-04-20T19:30:20.985623Z\",\"shell.execute_reply.started\":\"2025-04-20T19:30:20.967385Z\"}' execution_count=40}\n``` {.python .cell-code}\ntest_df.shape\n```\n:::\n\n\n::: {#fa3ec57f .cell execution='{\"iopub.execute_input\":\"2025-04-20T19:30:20.987328Z\",\"iopub.status.busy\":\"2025-04-20T19:30:20.986970Z\",\"iopub.status.idle\":\"2025-04-20T19:30:24.109891Z\",\"shell.execute_reply\":\"2025-04-20T19:30:24.109041Z\",\"shell.execute_reply.started\":\"2025-04-20T19:30:20.987305Z\"}' execution_count=41}\n``` {.python .cell-code}\ndisplay_img_captions(image_folder, n_samples = 10)\n```\n:::\n\n\n## Final comments\n\n1. Our model is doing a decent job of capturing the general idea of the images, but it struggles with specific details. It seems to specifically struggle with counting the number of actors or objects in a given image. Unfortunately, it also seems to occasionally hallucinate nonsensical words (e.g., \"minets\"). Considering how unique each of these images are (all the different variations of cameras, angles, lighting, actions and objects within the scene, etc.), and how our model is not specialized on any particular type or category of images, this unexceptional result is not surprising.\n2. To improve the performance of our image captioning model, we could consider the following:\n   1. **Model Architecture**: We could explore more advanced architectures, such as using a pre-trained transformer model like BERT or GPT-4.\n   2. **Training Dataset**: Using a larger and more diverse dataset could help the model learn better representations of images and captions. Alternatively, we could use a more specialized dataset to train our model to caption a specific type of image.\n   3. **Hyperparameter Tuning**: Experimenting with different hyperparameters, such as learning rate, batch size, and number of epochs, could help improve the model's performance. We could also try different optimizers or learning rate schedules to see if they yield better results.\n\nThanks to Dr. Varada Kolhatkar for her invaluable help and guidance throughout this project.\n\n",
    "supporting": [
      "transformer_captioning_files"
    ],
    "filters": [],
    "includes": {}
  }
}