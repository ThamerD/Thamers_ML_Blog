[
  {
    "objectID": "posts/transformer_captioning/transformer_captioning.html",
    "href": "posts/transformer_captioning/transformer_captioning.html",
    "title": "Image captioning using a transformer",
    "section": "",
    "text": "This post covers image captioning with a Transformer model, including image feature extraction, tokenization, positional encoding, model training, and generation.\nsource"
  },
  {
    "objectID": "posts/transformer_captioning/transformer_captioning.html#image-captioning-using-a-transformer",
    "href": "posts/transformer_captioning/transformer_captioning.html#image-captioning-using-a-transformer",
    "title": "Image captioning using a transformer",
    "section": "Image captioning using a transformer",
    "text": "Image captioning using a transformer\n\nCaption generation is the automated process of creating precise, grammatically sound descriptions for images. It requires recognizing essential objects, actions, and contextual elements within an image, then articulating them in natural language. This task bridges the fields of computer vision and natural language processing, leveraging a vision model to analyze image features and a language model to craft meaningful descriptive text.\nThe diagram above provides a high-level overview: a pre-trained computer vision model (such as a CNN) first extracts image features, which are then fed into a language model to generate captions sequentially, word by word."
  },
  {
    "objectID": "posts/transformer_captioning/transformer_captioning.html#imports",
    "href": "posts/transformer_captioning/transformer_captioning.html#imports",
    "title": "Image captioning using a transformer",
    "section": "Imports",
    "text": "Imports\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport math\nfrom PIL import Image as PIL_Image\nfrom tqdm import tqdm, trange\n\nTools we will use:\n\nPyTorch\nHugging Face Transformers\nPre-trained CNN models\n\nThe image captioning task consists of several key components, which we will divide into six distinct tasks: - TASK 1: Extract image features using pre-trained vision models. - TASK 2: Tokenize captions using pre-trained language models. - TASK 3: Create a PyTorch dataset for training. - TASK 4: Implement the ImageCaptioning class, which uses the transformer architecture for caption generation. - TASK 5: Write the PyTorch training loop to train the model. - TASK 6: Generate captions and evaluate the generated captions.\nWe will train the model using the flickr8k dataset. The training task will be computationally expensive. We may opt to use Kaggle notebooks, which offer free access to strong GPUs for up to 30 hours per week.\n\n# The folder containing the data \n# image_folder = \"flickr8k\"\n\n# If you run the notebook on Kaggle, you can use the following line\nimage_folder = \"/kaggle/input/flickr8k\"\n\n\n# Read and show the first few lines of caption.txt \ndf = pd.read_csv(f\"{image_folder}/captions.txt\", sep=',')\ndf.head()\n\n\n# Set the appropriate device depending upon your hardware. \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') \nprint(device)\n\n\nTASK 1: Extracting image features using pre-trained vision models\nThe initial step in the image captioning process involves extracting image features. This is commonly achieved by extracting the output from the final linear layer of a Convolutional Neural Network (CNN). Instead of building our own CNN from scratch, we will use a pre-trained CNN model.\n\nimport torch\nfrom transformers import AutoFeatureExtractor, ResNetModel\n\nclass ImageFeatureExtractor:\n    \"\"\"Extracts image features using a pretrained ResNet model.\"\"\"\n\n    def __init__(self, device, model_name=\"microsoft/resnet-18\"):\n        self.device = device\n        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n        self.resnet_model = ResNetModel.from_pretrained(model_name).to(device)\n        self.resnet_model.eval()\n\n    def __call__(self, image) -&gt; torch.Tensor:\n        \"\"\"Returns a (1, 512) tensor of features extracted from the input image.\"\"\"\n        \n        inputs = self.feature_extractor(images=image, return_tensors=\"pt\").to(self.device)\n\n        # Extract features using the ResNet model\n        with torch.no_grad():  # Disable gradient calculation\n            outputs = self.resnet_model(**inputs)\n\n        # Retrieve the features after the final pooling layer\n        features = outputs.pooler_output\n\n        # Ensure the feature shape is (1, 512)\n        features = features.view(1, -1)\n\n        # Return the features, moved back to CPU for processing compatibility\n        return features.cpu()\n\n\n# Now let's open an image \nimage = PIL_Image.open('{}/Images/{}'.format(image_folder, df.iloc[34396].image))\nimage\n\n\n\n\nimage.png\n\n\n\nfeature_extractor = ImageFeatureExtractor(device=device) # We'll need this later\n\n\n\nTASK 2: Implement a tokenizer using pre-trained language models\nNext, we’ll preprocess text data by tokenizing it using Hugging Face’s AutoTokenizer, specifically the bert-base-cased model. Since the vocabulary of bert-base-cased is significantly larger than that of our captions dataset, we’ll create a wrapper class for AutoTokenizer to define custom token-to-vocabulary mappings.\n\nfrom transformers import AutoTokenizer\nfrom tqdm import trange\n\nclass TokenizerWrapper:\n    \"\"\"Wraps AutoTokenizer with a custom vocabulary mapping.\"\"\"\n\n    def __init__(self, model_name=\"bert-base-cased\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        \n        # Initialize mappings with special tokens: [PAD] -&gt; 0, [CLS] -&gt; 1, [SEP] -&gt; 2\n        self.token_id_to_vocab_id = {0: 0, 101: 1, 102: 2}\n        self.vocab_id_to_token_id = {0: 0, 1: 101, 2: 102}\n        \n        self.vocab_id = 3  # Start after special tokens\n        self.padding_len = None\n\n    def build_dictionary(self, captions: list[str]):\n        \"\"\"Builds vocabulary from a list of captions and sets padding length.\"\"\"\n        tokenized = self.tokenizer(captions, padding='longest').input_ids\n        self.padding_len = len(tokenized[0])\n\n        for tokens in tokenized:\n            for token_id in tokens:\n                if token_id not in self.token_id_to_vocab_id:\n                    self.token_id_to_vocab_id[token_id] = self.vocab_id\n                    self.vocab_id_to_token_id[self.vocab_id] = token_id\n                    self.vocab_id += 1\n\n    def get_vocab_size(self) -&gt; int:\n        \"\"\"Returns the size of the custom vocabulary.\"\"\"\n        assert len(self.token_id_to_vocab_id) == len(self.vocab_id_to_token_id)\n        return self.vocab_id\n\n    def tokenize(self, text: str) -&gt; list[int]:\n        \"\"\"Tokenizes text using custom vocabulary (requires build_dictionary first).\"\"\"\n        assert self.padding_len is not None, \"Call build_dictionary() before tokenizing.\"\n        token_ids = self.tokenizer(text, padding='max_length', max_length=self.padding_len).input_ids\n        return [self.token_id_to_vocab_id[token_id] for token_id in token_ids]\n\n    def decode(self, vocab_ids: list[int]) -&gt; str:\n        \"\"\"Decodes a list of custom vocab IDs into a string.\"\"\"\n        token_ids = [self.vocab_id_to_token_id[vocab_id] for vocab_id in vocab_ids]\n        # Using `self.tokenizer.decode` to convert a list of token IDs back into a text string.\n        text = self.tokenizer.decode(token_ids)\n        return text.replace('[CLS] ', '').replace(' [SEP]', '').replace(' [PAD]', '')\n\n\n# Build the dictionary for our tokenizer  \ntokenizer_wrapper = TokenizerWrapper()\ntokenizer_wrapper.build_dictionary(df[\"caption\"].to_list())\n\n\n# What's the size of our custom vocabulary\ntokenizer_wrapper.get_vocab_size() \n\n\ntokenizer_wrapper.tokenizer.vocab_size\n\n\n# let's try to tokenize the caption corresponding to the image we saw in the last task, \n# and decode the tokens back to the caption\n\ncaption_tokens = tokenizer_wrapper.tokenize(df.iloc[34396].caption)\ndecoeded_caption = tokenizer_wrapper.decode(caption_tokens)\nprint('Caption:', df.iloc[34396].caption)\nprint('Tokens:', caption_tokens)\nprint('Decoded caption:', decoeded_caption)\n\n# Our Caption and the Decoded caption should match here. \n\n\n\nTASK 3: Data splitting and creating a Pytorch dataset\n\nPart 1: Data splitting\nUp to this point, we’ve developed an image feature extractor to generate feature vectors from images and a tokenizer to encode captions into meaningful representations. Now, it’s time to prepare our data for model training and evaluation.\nLet’s start by dividing the dataset into training and testing subsets.\n\nimport pandas as pd\nimport numpy as np\n\ndef train_test_split_by_image(data_df, sample_size=None, train_ratio=0.8, seed=100):\n    \"\"\"\n    Splits the dataframe into training and testing datasets based on unique images.\n    \n    Parameters:\n        data_df (pandas.DataFrame): The dataset to split, containing at least 'image' and 'caption' columns.\n        sample_size (int): The number of samples to consider. Useful during prototyping.\n        train_ratio (float): The proportion of the dataset to allocate to the training set.\n        seed (int): Seed for random number generator for reproducibility.\n\n    Returns:\n        train_df (pandas.DataFrame): Training dataset.\n        test_df (pandas.DataFrame): Testing dataset.\n    \"\"\"\n    np.random.seed(seed)\n    unique_images = np.random.permutation(data_df['image'].unique())\n\n    if sample_size:\n        unique_images = unique_images[:sample_size]\n\n    split_point = int(len(unique_images) * train_ratio)\n    train_images, test_images = unique_images[:split_point], unique_images[split_point:]\n\n    train_df = data_df[data_df['image'].isin(train_images)]\n    test_df = data_df[data_df['image'].isin(test_images)]\n\n    return train_df, test_df\n\n\n# Using a sample of the data for prototyping \ntrain_df, test_df = train_test_split_by_image(df, sample_size = 100, train_ratio=0.8)\nprint(f'The number of rows in the training set is {train_df.shape[0]} and the number of unique images is {int(train_df.shape[0]/5)}')\nprint(f'The number of rows in the test set is {test_df.shape[0]} and the number of unique images is {int(test_df.shape[0]/5)}')\ntrain_df.head()\n\n\n# Using the entire dataset\ntrain_df, test_df = train_test_split_by_image(df, train_ratio=0.8)\nprint(f'The number of rows in the training set is {train_df.shape[0]} and the number of unique images is {train_df.shape[0]/5}')\nprint(f'The number of rows in the test set is {test_df.shape[0]} and the number of unique images is {test_df.shape[0]/5}')\ntrain_df.head()\n\nNow that we have train and test datasets, the next steps involve processing each image and caption in both the training and testing sets while ensuring efficient batch loading and handling with PyTorch. To accomplish this, we’ll first write a function called build_data that converts the data into a torch tensor. After that, we’ll implement a class named PytorchDataset to construct a PyTorch dataset, making data management more streamlined.\n\n\nPart 2: creating a Pytorch dataset\n\ndef build_data(data_df, tokenizer_wrapper, feature_extractor, image_folder):\n    \"\"\"\n    Constructs a dataset list from provided image and caption data.\n\n    This function processes each entry in a dataframe, extracts image features using a specified\n    feature extractor, tokenizes captions using a tokenizer wrapper, and combines these elements into\n    a list where each item is a dictionary containing image features and tokenized caption data.\n\n    Parameters:\n        data_df (pandas.DataFrame): A dataframe containing at least two columns: 'image' and 'caption'.\n            - 'image' column contains filenames of images.\n            - 'caption' column contains the corresponding captions for the images.\n        tokenizer_wrapper (TokenizerWrapper): An object encapsulating a tokenizer that provides\n            a method 'tokenize' to convert text captions into token ids.\n        feature_extractor (Callable): A function or callable object that accepts an image object and\n            returns a tensor representing extracted features from the image.\n        image_folder (str): The path to the folder where images are stored. The path should not\n            end with a slash. It is assumed that images are stored within an 'Images' subfolder.\n\n    Returns:\n        list: A list of dictionaries, where each dictionary has two keys:\n            - 'image': a tensor containing features of the image.\n            - 'token': a tensor of token ids derived from the caption.\n\n    Example:\n        Assuming 'data_df' has two columns 'image' and 'caption' where 'image' contains filenames:\n        &gt;&gt;&gt; data_df = pd.DataFrame({\n        ...     'image': ['image1.jpg', 'image2.jpg'],\n        ...     'caption': ['A cat on a mat.', 'A dog in the fog.']\n        ... })\n        &gt;&gt;&gt; dataset = build_data(data_df, tokenizer_wrapper, feature_extractor, '/path/to/images')\n        &gt;&gt;&gt; print(dataset[0]['image'])  # Outputs the tensor of image features for 'image1.jpg'\n        &gt;&gt;&gt; print(dataset[0]['token'])  # Outputs the tensor of token ids for 'A cat on a mat.'\n    \"\"\"    \n\n    dataset = []\n    for i in trange(len(data_df)):\n        row = data_df.iloc[i]\n        image_path = f\"{image_folder}/Images/{row.image}\"\n        image = PIL_Image.open(image_path)\n\n        # Get image features \n        image_features = feature_extractor(image)\n\n        # Get caption tokens\n        caption_tokens = torch.tensor(tokenizer_wrapper.tokenize(row.caption))\n        \n        dataset.append({'image': image_features, 'token': caption_tokens})\n    return dataset\n\nLet’s create train and test datasets by calling build_data on train and test splits. Note: This will take some time to run, as we are processing all images and captions in the dataset.\n\ntrain_data = build_data(train_df, tokenizer_wrapper, feature_extractor, image_folder)\ntest_data = build_data(test_df, tokenizer_wrapper, feature_extractor, image_folder)\n\n\n# Get the dimension of the image feature\nimage_dim = train_data[0]['image'].shape[-1]\nvocab_size = tokenizer_wrapper.get_vocab_size()\nprint(f'Image feature dimension is {image_dim}. The vocab size is {vocab_size}.')\n\n# our image_dim should pass this test\nassert isinstance(image_dim, int)\n\nNow let’s buid our PyTorch dataset.\n\nclass PytorchDataset:\n    \"\"\"\n    A PyTorch-compatible dataset that returns:\n    - the original token sequence,\n    - image features,\n    - and the target sequence (input shifted left with padding at the end).\n    \"\"\"\n\n    def __init__(self, data, pad_vocab_id=0):\n        self.data = data\n        self.pad_token = torch.tensor([pad_vocab_id])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        tokens = self.data[idx]['token']\n        image = self.data[idx]['image']\n        target = torch.cat([tokens[1:], self.pad_token])\n        return tokens, image, target\n\n\ntrain_dataset = PytorchDataset(train_data)\ntest_dataset = PytorchDataset(test_data)\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n\n\n# Now let's get the first data from PytorchDataset\ntrain_token, train_image, train_target = train_dataset[0]\n\n# our dataset should pass these tests\nassert train_token.shape == train_target.shape\nassert torch.all(train_token[1:] == train_target[:-1]).item()\n\n\n# Shape of the image features\ntrain_image.shape\n\n\n# Numericalized caption\ntrain_token \n\n\n# decoded caption\ntokenizer_wrapper.decode(train_token.tolist()) \n\n\n# the target of the train_token (i.e., training example) \ntrain_target\n\n\n# decoded caption\ntokenizer_wrapper.decode(train_target.tolist()) \n\nThe decoded caption is the same for input and target because the special token [CLS] with token ID 1 is not displayed in the decoded caption.\n\n# Now let's get a batch of data from DataLoader\ntrain_text, train_image, train_target = next(iter(train_dataloader))\ntrain_text = train_text.to(device)\ntrain_image = train_image.to(device)\n\nassert train_image.ndim == 3\nassert train_text.ndim == 2\n\n\ntrain_text.shape # batch_size, max_len\n\n\ntrain_image.shape\n\n\ntrain_target.shape # batch_size, max_len\n\n\n\n\nTASK 4: Implementing the image captioning model using PyTorch\n\nTo summarize our model’s architecture, image features are first projected into the embedding space via self.image_embedding and then reshaped to serve as the decoder’s memory. Token IDs undergo embedding through self.text_embedding and are further enriched with positional information using self.pos_encoding. These processed inputs are then fed into the TransformerDecoder, where context-aware token representations are generated. The decoder’s output is passed through linear_layer to compute vocabulary logits, followed by a softmax function to derive token probabilities. During inference, a token is sampled from this distribution and used to generate the subsequent word.\nThe PositionalEncoding class adds sinusoidal positional encodings to input embeddings, helping the Transformer model understand word order in a sequence. Since Transformers lack inherent sequential processing, this encoding is crucial for generating structured captions in our image captioning task.\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"\n    Implements sinusoidal positional encoding as described in \"Attention is All You Need\".\n\n    Args:\n        d_model (int): Dimension of the embedding space.\n        dropout (float): Dropout rate after adding positional encodings.\n        max_len (int): Maximum length of supported input sequences.\n    \"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Create a (max_len, 1) position tensor: [[0], [1], ..., [max_len-1]]\n        positions = torch.arange(max_len).unsqueeze(1)\n\n        # Compute the scaling terms for each dimension (even indices only)\n        scale_factors = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n\n        # Initialize the positional encoding matrix with shape (max_len, 1, d_model)\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(positions * scale_factors)  # Apply sine to even indices\n        pe[:, 0, 1::2] = torch.cos(positions * scale_factors)  # Apply cosine to odd indices\n\n        # Register as buffer (not a trainable parameter)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Adds positional encoding to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, d_model)\n\n        Returns:\n            torch.Tensor: Tensor with positional encoding added.\n        \"\"\"\n        seq_len = x.size(0)\n        x = x + self.pe[:seq_len]\n        return self.dropout(x)\n\nNow we are ready to define our own transformer model tailored for image captioning!\n\nclass ImageCaptionModel(nn.Module):\n    def __init__(self, d_model, n_heads, num_layers, image_dim, vocab_size, device, dropout=0.1):\n        \"\"\"\n        Initialize the ImageCaptionModel which uses a transformer decoder architecture\n        for generating image captions.\n\n        Parameters:\n            d_model (int): The number of expected features in the encoder/decoder inputs.\n            n_heads (int): The number of heads in the multiheadattention models.\n            num_layers (int): The number of sub-decoder-layers in the transformer.\n            image_dim (int): The dimensionality of the input image features.\n            vocab_size (int): The size of the vocabulary.\n            device (torch.device): The device on which the model will be trained.\n            dropout (float): The dropout value used in PositionalEncoding and TransformerDecoderLayer.\n        \"\"\"        \n        super(ImageCaptionModel, self).__init__()\n        self.d_model = d_model\n        self.device = device\n        # Positional Encoding to add position information to input embeddings\n        self.pos_encoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n\n        # Here is our transformer architecture\n        \n        # Transformer Decoder to generate captions from image features and text inputs\n        self.TransformerDecoder = nn.TransformerDecoder(\n            decoder_layer=nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout), \n            num_layers=num_layers\n        )\n\n        # Linear layer to convert image features to a dimensionality that matches the model's\n        self.image_embedding = nn.Linear(image_dim , d_model)\n\n        # Embedding layer for converting input text tokens into vectors\n        self.text_embedding = nn.Embedding(vocab_size , d_model)\n\n        # Final linear layer to map the output of the transformer decoder to vocabulary size        \n        self.linear_layer = nn.Linear(d_model, vocab_size)\n\n        # Initialize the weights of the model\n        self.init_weights()\n        \n    def init_weights(self):\n        \"\"\"\n        Initialize weights of the model to small random values.\n        \"\"\"\n        initrange = 0.1\n        self.text_embedding.weight.data.uniform_(-initrange, initrange)\n        self.image_embedding.weight.data.uniform_(-initrange, initrange)\n        self.linear_layer.bias.data.zero_()\n        self.linear_layer.weight.data.uniform_(-initrange, initrange)\n        \n    def forward(self, image, text):\n        \"\"\"\n        Defines the forward pass of the model.\n\n        Parameters:\n            image (Tensor): The tensor containing image features.\n            text (Tensor): The tensor containing input text tokens.\n\n        Returns:\n            Tensor: The output tensor containing the logit scores for each vocabulary token.\n        \"\"\"                \n        # Convert image features to match the dimensionality of the model\n        encoded_image = self.image_embedding(image)\n        encoded_image = encoded_image.permute(1,0,2)\n\n        # Convert text tokens into embeddings and apply positional encoding\n        encoded_text = self.text_embedding(text) * math.sqrt(self.d_model)        \n        encoded_text = encoded_text.permute(1,0,2)        \n        encoded_text = self.pos_encoding(encoded_text)\n        \n        # Get the length of the sequences to be decoeded. This is needed to generate the causal masks\n        seq_len = encoded_text.size(0)  \n\n        # Generate a causal mask to prevent the model from attending to future tokens\n        causal_mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).transpose(0, 1)\n        causal_mask = causal_mask.float().masked_fill(causal_mask == 0, float('-inf')).masked_fill(causal_mask == 1, float(0.0))\n        causal_mask = causal_mask.to(self.device)\n\n        # Process through the transformer decoder\n        transformer_output = self.TransformerDecoder(tgt=encoded_text, memory=encoded_image, tgt_mask=causal_mask) \n\n        # Apply the final linear layer to produce predictions for each token\n        final_layer_output = self.linear_layer(transformer_output) \n        return final_layer_output\n\n\n# Now let's try the model. \n# Feel free to change these hyperparameters. \nd_model = 256 \nn_heads = 4\nnum_layers = 8\nmodel = ImageCaptionModel(d_model=d_model, n_heads=n_heads, num_layers=num_layers, image_dim=image_dim, vocab_size=vocab_size, device=device).to(device)\n\n\n# pass inputs to the model\noutput = model(train_image, train_text)\n\n# output should pass the test\nassert output.shape == (train_text.shape[1], train_text.shape[0], vocab_size)\n\n\n\nTASK 5: Implement the training loop\nThe next step is to train our ImageCaptionModel using the dataset we’ve prepared. We are defining the optimizer and the loss function below.\n\n# Feel free to change the hyperparameters. \n\nnum_epoch = 30\nclip_norm = 1.0\nlr = 5e-5\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=0) # Ignore the padding index\n\n\ndef trainer(model, criterion, optimizer, train_dataloader, test_dataloader, epochs=5, patience=5):\n    \"\"\"\n    Trains and evaluates a model using the specified dataloaders, optimizer, and loss criterion with early stopping.\n    \"\"\"    \n    train_losses = []\n    test_losses = []\n    consec_increases = 0        \n    \n    for epoch in range(epochs):\n        sum_train_loss = 0\n        sum_test_loss = 0\n        num_train_loss = 0        \n        num_test_loss = 0\n\n        # Set model to training mode\n        model.train()\n\n        for train_text, train_image, target_seq in train_dataloader:\n\n            train_text = train_text.to(device)\n            train_image = train_image.to(device)\n            target_seq = target_seq.to(device)\n\n            # Forward pass\n            output = model(train_image, train_text) \n            output = output.permute(1, 2, 0) \n            train_loss = criterion(output, target_seq) \n\n            # Backward pass \n            optimizer.zero_grad()        \n            train_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n            optimizer.step()\n\n            # Book keeping \n            sum_train_loss += torch.sum(train_loss).detach().item()\n            num_train_loss += 1 \n\n        # Set the model to evaluation mode\n        model.eval()\n        with torch.no_grad():\n            for test_text, test_image, target_seq in test_dataloader:\n                test_text = test_text.to(device)\n                test_image = test_image.to(device)\n                target_seq = target_seq.to(device)\n\n                # Forward pass\n                output = model(test_image, test_text) \n                output = output.permute(1, 2, 0) \n                test_loss = criterion(output, target_seq)\n                \n                # Book keeping \n                sum_test_loss += torch.sum(test_loss).detach().item()\n                num_test_loss += 1\n\n        # Calculate and store average losses\n        avg_train_loss = sum_train_loss / num_train_loss\n        avg_test_loss = sum_test_loss / num_test_loss\n        train_losses.append(avg_train_loss)\n        test_losses.append(avg_test_loss)\n        print(f\"Epoch {epoch+1}: Train Loss {avg_train_loss:.2f}, Test Loss {avg_test_loss:.2f}\")\n\n        # Early stopping\n        if epoch &gt; 0 and test_losses[-1] &gt; test_losses[-2]:\n            consec_increases += 1\n        else:\n            consec_increases = 0\n        if consec_increases == patience:\n            print(f\"Stopped early at epoch {epoch + 1} - test loss increased for {consec_increases} consecutive epochs!\")\n            break\n            \n    return train_losses, test_losses\n\n\n# Warning: This may take a while to run.\ntrain_losses, test_losses = trainer(model, criterion, optimizer,train_dataloader, test_dataloader, epochs= num_epoch)\n\n\n\nTASK 6: Generate captions using the trained model\nNow that we have trained our model, the moment of truth has arrived! The final task is generating captions using ImageCaptionModel.\n\ndef get_test_data(index): \n    \"\"\"\n    Retrieve the text and image data from the test dataset at the specified index.\n\n    Parameters:\n        index (int): The index of the test dataset from which to fetch the data.\n\n    Returns:\n        tuple: A tuple containing:\n            - test_text (Tensor): The tensor representing the caption.\n            - test_image (Tensor): The tensor representing the image.\n    \"\"\"    \n    test_text, test_image, target_seq = test_dataset[index]\n    return test_text, test_image\n    \ndef generate_caption(model, image, device, max_caption_length=100, start_vocab=1, end_vocab=2):\n    \"\"\"\n    Generates a caption for an image using the specified model and device.\n\n    Parameters:\n        model (torch.nn.Module): The trained model used for generating captions.\n        image (torch.Tensor): The image tensor for which to generate the caption.\n        device (torch.device): The device (e.g., CPU or GPU) to which tensors will be sent for model execution.\n        max_caption_length (int, optional): The maximum length of the generated caption. Defaults to 100.\n        start_vocab (int, optional): The vocabulary index used to signify the start of a caption. Defaults to 1.\n        end_vocab (int, optional): The vocabulary index used to signify the end of a caption. Defaults to 2.\n        \n    Returns:\n        numpy.ndarray: An array containing the sequence of vocabulary indices representing the generated caption.\n        \n    \"\"\"    \n    image = torch.unsqueeze(image, dim=0).to(device)\n    context = torch.tensor([[start_vocab]]).to(device)\n    for _ in range(max_caption_length):\n        logits = model(image, context)[-1]\n        probabilities = torch.softmax(logits, dim=-1).flatten(start_dim=1)\n        next_vocab = torch.multinomial(probabilities, num_samples=1)\n        context = torch.cat([context, next_vocab], dim=1)\n        if next_vocab.item() == end_vocab:\n            break\n    return context.cpu().numpy().flatten()\n\n\nimport random \n\nrandom.seed(10)\n\ndef display_img_captions(image_folder, n_samples = 5, indices=None):\n    \n    # Set a fixed size for images\n    desired_size = (200, 200)\n\n    # If no specific indices are provided, select random samples\n    if indices == None:\n        indices = random.sample(range(test_df.shape[0]), n_samples)\n        print(indices)\n    \n    for index in indices: \n        test_text, test_image = get_test_data(index=index)\n        generated_vocab = generate_caption(model, test_image, device, max_caption_length=100)\n        generated_caption = tokenizer_wrapper.decode(generated_vocab)\n        gold_caption = tokenizer_wrapper.decode(test_text.numpy())\n\n        # Load and prepare the image\n        image_path = '{}/Images/{}'.format(image_folder, test_df.iloc[index]['image'])\n        image = PIL_Image.open(image_path)\n        img = image.resize(desired_size)\n        \n        # Display the image\n        plt.figure(figsize=(5, 5))  # Create a new figure for each image\n        plt.imshow(img)\n        plt.axis('off')  # Turn off axis numbers and ticks\n        plt.title(f\"Generated: {generated_caption}\\nActual: {gold_caption}\")\n        plt.show()\n        \n        # print('Generated: ', generated_caption)\n        # print('Actual: ', gold_caption)\n        print('\\n\\n')\n\n\ntest_df.shape\n\n\ndisplay_img_captions(image_folder, n_samples = 10)"
  },
  {
    "objectID": "posts/transformer_captioning/transformer_captioning.html#final-comments",
    "href": "posts/transformer_captioning/transformer_captioning.html#final-comments",
    "title": "Image captioning using a transformer",
    "section": "Final comments",
    "text": "Final comments\n\nOur model is doing a decent job of capturing the general idea of the images, but it struggles with specific details. It seems to specifically struggle with counting the number of actors or objects in a given image. Unfortunately, it also seems to occasionally hallucinate nonsensical words (e.g., “minets”). Considering how unique each of these images are (all the different variations of cameras, angles, lighting, actions and objects within the scene, etc.), and how our model is not specialized on any particular type or category of images, this unexceptional result is not surprising.\nTo improve the performance of our image captioning model, we could consider the following:\n\nModel Architecture: We could explore more advanced architectures, such as using a pre-trained transformer model like BERT or GPT-4.\nTraining Dataset: Using a larger and more diverse dataset could help the model learn better representations of images and captions. Alternatively, we could use a more specialized dataset to train our model to caption a specific type of image.\nHyperparameter Tuning: Experimenting with different hyperparameters, such as learning rate, batch size, and number of epochs, could help improve the model’s performance. We could also try different optimizers or learning rate schedules to see if they yield better results.\n\n\nThanks to Dr. Varada Kolhatkar for her invaluable help and guidance throughout this project."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html",
    "href": "posts/decision_trees/decision_trees.html",
    "title": "Machine Learning Models: Decision Trees",
    "section": "",
    "text": "On this page, we explore Decision Trees and showcase one of their use cases."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#imports",
    "href": "posts/decision_trees/decision_trees.html#imports",
    "title": "Machine Learning Models: Decision Trees",
    "section": "Imports",
    "text": "Imports\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score, cross_validate, train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\nWe’ll be using Kaggle’s Spotify Song Attributes dataset. The dataset contains a number of features of songs from 2017 and a binary variable target that represents whether the user liked the song (encoded as 1) or not (encoded as 0). See the documentation of all the features here."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#read-and-split-the-dataset",
    "href": "posts/decision_trees/decision_trees.html#read-and-split-the-dataset",
    "title": "Machine Learning Models: Decision Trees",
    "section": "1. Read and split the dataset",
    "text": "1. Read and split the dataset\n\n\nWe use read_csv from the pandas package to read the data.\n\nWe use train_test_split from sklearn to split the data into separate training and test sets. (Not to be confused with validation sets which will be created later from the training set).\n\nThe test_size parameter determines the proportion of the test set to the training set. Generally, a larger training set results in a better model, a larger test set results in a more accurate assessment of the model. We must find a balance between these two.\nNote that the dataset is sorted on the target. If we maintain this list sorting our model will simply predict the target based on the song’s position in the sorted list, rather than its features. This will not help us make predictions for future unseen data. Therefore, we set the first column as the index so that our model does not learn the sorted order of our data.\n\n\n\nspotify_df = pd.read_csv(\"data/spotify.csv\", index_col=0)\n\nspotify_df.head() # to show a sample from the dataset\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nduration_ms\nenergy\ninstrumentalness\nkey\nliveness\nloudness\nmode\nspeechiness\ntempo\ntime_signature\nvalence\ntarget\nsong_title\nartist\n\n\n\n\n0\n0.0102\n0.833\n204600\n0.434\n0.021900\n2\n0.1650\n-8.795\n1\n0.4310\n150.062\n4.0\n0.286\n1\nMask Off\nFuture\n\n\n1\n0.1990\n0.743\n326933\n0.359\n0.006110\n1\n0.1370\n-10.401\n1\n0.0794\n160.083\n4.0\n0.588\n1\nRedbone\nChildish Gambino\n\n\n2\n0.0344\n0.838\n185707\n0.412\n0.000234\n2\n0.1590\n-7.148\n1\n0.2890\n75.044\n4.0\n0.173\n1\nXanny Family\nFuture\n\n\n3\n0.6040\n0.494\n199413\n0.338\n0.510000\n5\n0.0922\n-15.236\n1\n0.0261\n86.468\n4.0\n0.230\n1\nMaster Of None\nBeach House\n\n\n4\n0.1800\n0.678\n392893\n0.561\n0.512000\n5\n0.4390\n-11.648\n0\n0.0694\n174.004\n4.0\n0.904\n1\nParallel Lines\nJunior Boys\n\n\n\n\n\n\n\n\ntrain_df = None\ntest_df = None\n\ntrain_df, test_df = train_test_split(\n    spotify_df, test_size=0.2, random_state=123\n)"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#exploratory-data-analysis-eda",
    "href": "posts/decision_trees/decision_trees.html#exploratory-data-analysis-eda",
    "title": "Machine Learning Models: Decision Trees",
    "section": "2. Exploratory Data Analysis (EDA)",
    "text": "2. Exploratory Data Analysis (EDA)\nIn this section, we want to take a closer look at the dataset so that we can make more informed decisions when designing the model later.\n\n\nn_train_samples = train_df.shape[0]\nn_test_samples = test_df.shape[0]\n\nprint(f\"Number of training samples: {n_train_samples}\")\nprint(f\"Number of test samples: {n_test_samples}\")\n\nNumber of training samples: 1613\nNumber of test samples: 404\n\n\n\nspotify_summary = train_df.describe()\nspotify_summary\n\n\n\n\n\n\n\n\nacousticness\ndanceability\nduration_ms\nenergy\ninstrumentalness\nkey\nliveness\nloudness\nmode\nspeechiness\ntempo\ntime_signature\nvalence\ntarget\n\n\n\n\ncount\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n1613.000000\n\n\nmean\n0.185627\n0.616745\n247114.827650\n0.681296\n0.136862\n5.383137\n0.189189\n-7.112929\n0.621203\n0.091277\n121.979777\n3.964662\n0.497587\n0.507750\n\n\nstd\n0.259324\n0.163225\n81177.300308\n0.211612\n0.277744\n3.620422\n0.153170\n3.838867\n0.485238\n0.087890\n26.965641\n0.255201\n0.247378\n0.500095\n\n\nmin\n0.000005\n0.122000\n16042.000000\n0.014800\n0.000000\n0.000000\n0.018800\n-33.097000\n0.000000\n0.023100\n47.859000\n1.000000\n0.035900\n0.000000\n\n\n25%\n0.009190\n0.511000\n200105.000000\n0.564000\n0.000000\n2.000000\n0.092300\n-8.388000\n0.000000\n0.037300\n100.518000\n4.000000\n0.295000\n0.000000\n\n\n50%\n0.062500\n0.629000\n230200.000000\n0.714000\n0.000071\n6.000000\n0.127000\n-6.248000\n1.000000\n0.054900\n121.990000\n4.000000\n0.496000\n1.000000\n\n\n75%\n0.251000\n0.738000\n272533.000000\n0.844000\n0.057300\n9.000000\n0.243000\n-4.791000\n1.000000\n0.107000\n137.932000\n4.000000\n0.690000\n1.000000\n\n\nmax\n0.995000\n0.984000\n849960.000000\n0.997000\n0.976000\n11.000000\n0.969000\n-0.307000\n1.000000\n0.816000\n219.331000\n5.000000\n0.992000\n1.000000\n\n\n\n\n\n\n\nIn the following plots, we explore different features and analyze their relationship with our target. 1 means the user liked the song, 0 means they did not.\n\n# Histogram for loudness\nfeat = \"loudness\"\ntrain_df.groupby(\"target\")[feat].plot.hist(bins=50, alpha=0.5, legend=True, density = True, title = \"Histogram of \" + feat)\nplt.xlabel(feat)\n\nText(0.5, 0, 'loudness')\n\n\n\n\n\n\n\n\n\n\nfor feat in ['acousticness', 'danceability', 'tempo', 'energy', 'valence']: # This loop creates a histogram for each of the features in the list\n    train_df.groupby(\"target\")[feat].plot.hist(bins=50, alpha=0.5, legend=True, density = True, title = \"Histogram of \" + feat)\n    plt.xlabel(feat)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKeep in mind that even if we see a feature with a histogram that has not discernable patterns with the target, it does not necessarily mean that the feature is not useful for predicting the target. As some patterns only appear when a feature is combined with another. For example: Valence on its own seems insignificant for predicting the target, but that can change when we look at Valence alongside Tempo.\nNote that the dataset includes two text features labeled song_title and artist. For now, we will simply drop these text features as encoding text can be tricky and may derail us from our original goal here, which is to explore decision trees."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#select-features",
    "href": "posts/decision_trees/decision_trees.html#select-features",
    "title": "Machine Learning Models: Decision Trees",
    "section": "3. Select features",
    "text": "3. Select features\n\nIn this section, we select the features we want our model to learn. In our case, we will take all the available features except for song_title and artist. Note that we also need to split our x and y (features and target respectively).\n\nX_train = train_df.drop(columns=['target', 'song_title', 'artist'])\ny_train = train_df['target']\nX_test = test_df.drop(columns=['target', 'song_title', 'artist'])\ny_test = test_df['target']"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#create-and-assess-the-baseline",
    "href": "posts/decision_trees/decision_trees.html#create-and-assess-the-baseline",
    "title": "Machine Learning Models: Decision Trees",
    "section": "4. Create and assess the baseline",
    "text": "4. Create and assess the baseline\n\nIn this section, we create a very simple baseline model which we will use to measure our decision tree model against. In our case, the DummyClassifier will simply predict the most frequent case. Meaning if most songs in our dataset were liked, it will predict that they were all liked.\nWe then use cross_val_score to assess our baseline model.\n\ndum = DummyClassifier(random_state=123, strategy='most_frequent')\ndummy_score = np.mean(cross_val_score(dum, X_train, y_train, cv=10))\ndummy_score\n\n0.5077524729698643"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#create-and-assess-the-decision-tree-model",
    "href": "posts/decision_trees/decision_trees.html#create-and-assess-the-decision-tree-model",
    "title": "Machine Learning Models: Decision Trees",
    "section": "5. Create and assess the Decision Tree model",
    "text": "5. Create and assess the Decision Tree model\n\nIn this section, we finally create the decision tree model, and we assess it using cross_validate. Note that this function fits the model to the dataset as its first step so we don’t need to fit our model beforehand.\n\nspotify_tree = DecisionTreeClassifier(random_state=123)\n\n\ndt_scores_df = pd.DataFrame(cross_validate(spotify_tree, X_train, y_train, cv=10, return_train_score=True))\ndt_scores_df\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_score\ntrain_score\n\n\n\n\n0\n0.010071\n0.000000\n0.697531\n0.999311\n\n\n1\n0.010068\n0.001000\n0.660494\n1.000000\n\n\n2\n0.010107\n0.001001\n0.685185\n0.999311\n\n\n3\n0.011071\n0.000000\n0.639752\n1.000000\n\n\n4\n0.009850\n0.001000\n0.639752\n0.999311\n\n\n5\n0.009578\n0.001065\n0.658385\n0.999311\n\n\n6\n0.009064\n0.001000\n0.639752\n0.999311\n\n\n7\n0.010087\n0.000999\n0.608696\n0.999311\n\n\n8\n0.010071\n0.000000\n0.701863\n0.999311\n\n\n9\n0.010067\n0.001000\n0.695652\n0.999311\n\n\n\n\n\n\n\nThe main number we want to look at here is test_score. We ran 10 different tests on our model, let’s take their mean value and compare it to our baseline.\n\nround(dt_scores_df['test_score'].mean(), 3)\n\n0.663"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#optional-visualize-the-model",
    "href": "posts/decision_trees/decision_trees.html#optional-visualize-the-model",
    "title": "Machine Learning Models: Decision Trees",
    "section": "6. (Optional) Visualize the model",
    "text": "6. (Optional) Visualize the model\n\nIn this section, we use the tree package to visualize our decision tree model to understand it better\n\nspotify_tree.fit(X_train, y_train) # We must fit (train) the model before we visualize it\n\nfeature_names = X_train.columns.tolist() # feature names \nclass_names = [\"Liked\", \"Disliked\"] # unique class names \n\ntoy_tree_viz = tree.plot_tree(spotify_tree, feature_names=feature_names, class_names=class_names, max_depth=1)\n# The tree is too big and complicated to fully visualize, so we set max_depth=2 to visualize the first layers only"
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#hyperparameter-optimization",
    "href": "posts/decision_trees/decision_trees.html#hyperparameter-optimization",
    "title": "Machine Learning Models: Decision Trees",
    "section": "6. Hyperparameter optimization",
    "text": "6. Hyperparameter optimization\n\nSo far, we have used the decision tree model in its default configuration and got some decent results. But how can we make it perform better? We need to optimize its hyperparameters. In our case, the decision tree model has a single hyperparameter depth which determines the depths of the decision tree.\nLet’s try out a number of different depths and see which one preforms best.\n\ndepths = np.arange(1, 25, 2)\ndepths\n\narray([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19, 21, 23])\n\n\n\nresults_dict = {\n    \"depth\": [],\n    \"mean_train_score\": [],\n    \"mean_cv_score\": [],\n}\n\nfor depth in depths: # Create a model for each depth in our list, assess it, and add it to our results_df\n    model = DecisionTreeClassifier(max_depth=depth, random_state=123)\n    scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n    results_dict[\"depth\"].append(depth)\n    results_dict[\"mean_cv_score\"].append(np.mean(scores[\"test_score\"]))\n    results_dict[\"mean_train_score\"].append(np.mean(scores[\"train_score\"]))\n\nresults_df = pd.DataFrame(results_dict)\nresults_df = results_df.set_index(\"depth\")\nresults_df\n\n\n\n\n\n\n\n\nmean_train_score\nmean_cv_score\n\n\ndepth\n\n\n\n\n\n\n1\n0.651030\n0.646032\n\n\n3\n0.733485\n0.692524\n\n\n5\n0.794035\n0.711713\n\n\n7\n0.858718\n0.703060\n\n\n9\n0.912930\n0.690610\n\n\n11\n0.955157\n0.680048\n\n\n13\n0.980850\n0.674457\n\n\n15\n0.993525\n0.658979\n\n\n17\n0.998278\n0.669538\n\n\n19\n0.999173\n0.665812\n\n\n21\n0.999449\n0.662706\n\n\n23\n0.999449\n0.662706\n\n\n\n\n\n\n\nWe can see that in our case, depth 5 yields the best result: 0.711713. However, we must also consider the fundamental tradeoff. We want our model to have the highest test scores, but if its training score is too high it may suggest that it is overfitting on our particular dataset and will generalize poorly to future unseen data. To take a closer look at this, let’s plot our model’s scores and see how they change as depth changes.\n\nresults_df[[\"mean_train_score\", \"mean_cv_score\"]].plot()\n\n\n\n\n\n\n\n\n\n\nWe can see that the mean_cv_score peaks at depth 5 then begins to decrease. Whereas the mean_train_score continuously increases. We can conclude that depth 5 is the ideal depth for our model in this use case. This is what we call “The sweet spot”."
  },
  {
    "objectID": "posts/decision_trees/decision_trees.html#final-model-and-test",
    "href": "posts/decision_trees/decision_trees.html#final-model-and-test",
    "title": "Machine Learning Models: Decision Trees",
    "section": "7. Final model and test",
    "text": "7. Final model and test\n\nIn this section, we recreate our decision tree model using the optimized hyperparameter, then we test it and compare our results with out unoptimized and baseline models.\n\nbest_model = DecisionTreeClassifier(max_depth=5, random_state=123)\nbest_model.fit(X_test, y_test)\ntest_score = best_model.score(X_test, y_test)\ntest_score\n\n0.8267326732673267\n\n\nTo recap: - Baseline model score: ~0.51\n- Unoptimized decision tree model score: ~0.67\n- Optimized decision tree model score: ~0.83"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thamer's ML Blog",
    "section": "",
    "text": "From Application to Graduation: Uncovering Patterns of Venture Success\n\n\n\n\n\n\n\n\n\n\n\nAbeba Turi, Dhruv Garg, Ethan Fang, Thamer Aldawood\n\n\n\n\n\n\n\n\n\n\n\n\nImage captioning using a transformer\n\n\n\n\n\n\nPyTorch\n\n\nNLP\n\n\nLLM\n\n\nTransformers\n\n\n\n\n\n\n\n\n\nMay 18, 2025\n\n\nThamer Aldawood\n\n\n\n\n\n\n\n\n\n\n\n\nCat breed CNN classifier\n\n\n\n\n\n\nML Models\n\n\nGuides\n\n\nPyTorch\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nThamer Aldawood\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning Models: Decision Trees\n\n\n\n\n\n\nGuides\n\n\nAnalysis\n\n\nscikit-learn\n\n\n\n\n\n\n\n\n\nFeb 22, 2025\n\n\nThamer Aldawood\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Machine Learning - Beginner’s Guide\n\n\n\n\n\n\nGuides\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nJan 15, 2025\n\n\nThamer Aldawood\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Thamer Aldawood. This blog is where I will share the lessons I learn throughout my ongoing Machine Learning journey.\nI have worked as a Business Analyst and Software Engineer for 3+ years, then decided to take on the Master of Data Science program from UBC to expand my knowledge of Machine Learning, Artificial Intelligence, and the mechanisms working behind the scenes that enable them to do the incredible things they do."
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html",
    "title": "Cat breed CNN classifier",
    "section": "",
    "text": "This is a Convolutional Neural Network (CNN) that classifies cats based on their images into one of 6 breeds: 1. American Short hair\n2. Bengal\n3. Maine Soon\n4. Ragdoll\n5. Scottish Fold\n6. Sphinx\nWe will leverage PyTorch for this task."
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#preamble-the-dataset",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#preamble-the-dataset",
    "title": "Cat breed CNN classifier",
    "section": "Preamble: The dataset",
    "text": "Preamble: The dataset\nWe will be using the following public Kaggle dataset: https://www.kaggle.com/datasets/solothok/cat-breed It contains training data which has 200 images for each class and test data that has 50 images for each class."
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#imports",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#imports",
    "title": "Cat breed CNN classifier",
    "section": "Imports",
    "text": "Imports\n\n\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\nimport torch\nfrom torch import nn, optim\nfrom torchvision import datasets, transforms, utils, models\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfrom PIL import Image\n\nplt.rcParams.update({'axes.grid': False})\n\nCNNs are comupationally demanding to run. Ideally, we want to utilize a GPU to improve our performance instead of a CPU. The following code snippet checks that our GPU and Pytorch setup is working.\n\ntorch.cuda.is_available()\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Use GPU (CUDA) if available, else use CPU\nprint(f\"Using device: {device.type}\")"
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#cnn-from-scratch",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#cnn-from-scratch",
    "title": "Cat breed CNN classifier",
    "section": "1. CNN from scratch",
    "text": "1. CNN from scratch\nFirst, let’s try to make a CNN from scratch for this task, and see how well it performs (spoiler: it won’t be anywhere near as good as pre-trained models). This will also helps us understand how CNNs work and how we can adapt them to different tasks.\n\n1.1 Read the data\nWe simply need to set the paths to our training and test sets. Keep in mind that we are not actually reading the data yet, just learning where it is.\n\nimport torchvision\n\nTRAIN_DIR = \"/kaggle/input/cat-breed/cat-breed/TRAIN\"\nTEST_DIR = \"/kaggle/input/cat-breed/cat-breed/TEST\"\n\n\nprint(f\"Classes: {train_dataset.classes}\")\nprint(f\"Class count: {train_dataset.targets.count(0)}, {train_dataset.targets.count(1)}\")\nprint(f\"Samples:\",len(train_dataset))\nprint(f\"First sample: {train_dataset.samples[0]}\")\n\n\n\n1.2 Transform images\nThe images in our dataset have all kinds of different resolutions and aspect ratios, we must normalize them to a specific shape and size to be able to work with them. We will tranform them into 200x200 images.\n\nIMAGE_SIZE = (200, 200) # This does not depend on the size of the raw images\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=data_transforms)\ntest_dataset = torchvision.datasets.ImageFolder(root=TEST_DIR, transform=data_transforms)\n\n\n\n1.3 Create batches\nOur dataset is too large to be loaded all at once, we must instead create loaders to feed the data into our model in batches.\n\nBATCH_SIZE = 64\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,          \n    batch_size=BATCH_SIZE,  \n    shuffle=True,           \n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,          \n    batch_size=BATCH_SIZE,  \n    shuffle=True,          \n)\n\n\n# (Optional) Checking our batches\n\nimgs, targets = next(iter(train_loader))\n\nprint(f\"  Number of batches: {len(train_loader)}\")\nprint(f\"    Image data type: {type(imgs)}\")\nprint(f\"   Image batch size: {imgs.shape}\") \nprint(f\"  Target batch size: {targets.shape}\")\n\n\n# A sample of our training data\n\nsample_batch = next(iter(train_loader))\nplt.figure(figsize=(10, 8)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\nplt.imshow(np.transpose(torchvision.utils.make_grid(sample_batch[0], padding=1, normalize=True),(1,2,0)));\n\n\n\n1.4 Create the CNN\nNow, the fun part. We design a CNN using various functions from the Pytorch package. There is no right or wrong here, we simply experiment with different configuration until we reach a suitabel model.\n\nimport torch\nimport torch.nn as nn\n\nclass cat_CNN(nn.Module):\n    def __init__(self):\n        super(cat_CNN, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Dropout(0.2),\n\n            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Dropout(0.5),\n\n            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Dropout(0.2),\n\n            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n            nn.Dropout(0.2),\n\n            nn.Flatten(),\n            \n            nn.Linear(256 * 12 * 12, 512),\n            nn.ReLU(),\n\n            nn.Linear(512, 128),\n            nn.ReLU(),\n\n            nn.Linear(128, 6)\n        )\n        \n    def forward(self, x):\n        return self.model(x)\n\n\n\n1.5 Train the model\nIt is time to train the model. In the function below, not only do we train the model, we also evaluate it after every epoch. This is useful for continuously observing the performance of our model, and can help us decide if we need to change the number of epochs, or adjust certain aspects of our model that may be causing overfitting, etc.\nNote: Training CNNs is computationally demanding. I am using Kaggle’s GPU T4 x 2 accelerator to improve performance.\n\ndef trainer(model, criterion, optimizer, trainloader, validloader, epochs=5, verbose=True):\n    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n    \n    train_loss, valid_loss, valid_accuracy = [], [], []\n    for epoch in range(epochs):  # for each epoch\n        train_batch_loss = 0\n        valid_batch_loss = 0\n        valid_batch_acc = 0\n        \n        # Training\n        model.train()\n        for X, y in trainloader:\n            X, y = X.to(device), y.to(device)\n            optimizer.zero_grad()\n            y_hat = model(X)\n            loss = criterion(y_hat, y)\n            loss.backward()\n            optimizer.step()\n            train_batch_loss += loss.item()\n        train_loss.append(train_batch_loss / len(trainloader))\n        \n        # Validation\n        model.eval()\n        \n        with torch.no_grad():  # this stops pytorch doing computational graph stuff under-the-hood\n            for X, y in validloader:\n                X, y = X.to(device), y.to(device)\n                y_hat = model(X)\n                _, y_hat_labels = torch.softmax(y_hat, dim=1).topk(1, dim=1)\n                loss = criterion(y_hat, y)\n                valid_batch_loss += loss.item()\n                valid_batch_acc += (y_hat_labels.squeeze() == y).type(torch.float32).mean().item()\n        valid_loss.append(valid_batch_loss / len(validloader))\n        valid_accuracy.append(valid_batch_acc / len(validloader))  # accuracy\n        \n\n        \n        # Print progress\n        if verbose:\n            print(f\"Epoch {epoch + 1}:\",\n                  f\"Train Loss: {train_loss[-1]:.3f}.\",\n                  f\"Valid Loss: {valid_loss[-1]:.3f}.\",\n                  f\"Valid Accuracy: {valid_accuracy[-1]:.2f}.\")\n    \n    results = {\"train_loss\": train_loss,\n               \"valid_loss\": valid_loss,\n               \"valid_accuracy\": valid_accuracy}\n    return results    \n\n\n# Training the model and observing its results\n\ncat_model = cat_CNN().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(cat_model.parameters(), lr=1e-3)\n\ntorch.manual_seed(792)\n\nresults = trainer(cat_model, criterion, optimizer, train_loader, test_loader, epochs=20)\n\nWe can see that our humble from-scratch model is able to correctly classify cat breeds approximately half the time. We can do a lot better by leveraging pre-trained models as we will explore in the next section."
  },
  {
    "objectID": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#pre-trained-models",
    "href": "posts/cat_breed_cnn_classifier/cat_breed_cnn_classifier.html#pre-trained-models",
    "title": "Cat breed CNN classifier",
    "section": "2. Pre-trained models",
    "text": "2. Pre-trained models\nWe will explore how we can leverage pre-trained models to improve our model’s performance. We will use DenseNet to extract features from our images, then feed it to our model to finally classify them.\n\n2.1 Pre-trained model as-is\nThe simplest way to use a pre-trained model is to use it as-is, let’s try that and see how it performs then decide whether or not we want to fine-tune it for our use case.\n\ndensenet = models.densenet121(pretrained=True)\n\nfor param in densenet.parameters():  # Freeze parameters so we don't update them (Keep model as-is)\n    param.requires_grad = False\n\ndensenet.classifier\n\n\n# Our classification layer\ncustom_classification_layer = nn.Sequential(\n    nn.Linear(1024, 50),\n    nn.ReLU(),\n    nn.Linear(50, 6)\n)\n\ndensenet.classifier = custom_classification_layer\n\ndensenet.classifier\n\nNote: The DenseNet model is much larger and more complex than our scratch-made model. It is very computationally demanding.\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device.type}\")\n\ndensenet.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(densenet.parameters(), lr=1e-3)\n\n\nresults = trainer(densenet, criterion, optimizer, train_loader, test_loader, epochs=10)\n\nAfter leveraging the pre-trained model, our performance has become very impressive. Our model can now correctly classify cat breeds based on their images 92% of the time.\n\n\n2.2: Fine-tuned pre-trained model\nThe DenseNet model is incredible, but it is forced to make certain compromises to generalize for different tasks. Let’s try to fine-tune it so that it performs better for our specific cat breed classification task.\nThere are plenty of ways to fine-tune a pre-trained model. In our case, we will explore 2 fine-tuning methods:\n1. Fully unfrozen pre-trained model. 2. Partially unfrozen pre-trained model.\n\n2.2.1 Fully unfrozen pre-trained model\nThis fine-tuning method throws away all of the learned parameters from the pre-trained model, and generates new ones by training it on our dataset. Essentially, it uses the architecture of the pre-trained model but throws away all of its previous training.\n\ndensenet_ft_full = models.densenet121(pretrained=True)\nfor param in densenet_ft_full.parameters():\n    # Freeze parameters so we don't update them\n    param.requires_grad = False\n\ndensenet_ft_full.classifier = custom_classification_layer # replace classification layer with ours\n\n\ndensenet_ft_full.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(densenet_ft_full.parameters(), lr=1e-3)\nresults = trainer(densenet_ft_full, criterion, optimizer, train_loader, test_loader, epochs=10)\n\nWe can see that the fully unfrozen pre-trained model performs very similarly to the as-is pre-trained model. This is mostly coincidental. Let’s see if we can do better.\n\n\n2.2.2 Partially unfrozen pre-trained model\nThis fine-tuning method throws keeps almost everything from the pre-trained model, but fine-tunes a few of its layers (usually the last) to work better for our use case. These models should take less time to train as we’re only training a few layers rather than training all of them.\n\ndensenet_ft_partial = models.densenet121(pretrained=True)\nfor layer in densenet_ft_partial.features[:-1]: # Freezing all but last layer\n    for param in layer.parameters():\n        param.requires_grad = False\n\ndensenet_ft_partial.classifier = custom_classification_layer\n\n\n# Train the model\ndensenet_ft_partial.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(densenet_ft_partial.parameters(), lr=1e-3)\nresults = trainer(densenet_ft_partial, criterion, optimizer, train_loader, test_loader, epochs=10)\n\n\n\n\nRecap:\n\nFrom-scratch model performance: ~0.54 validation accuracy.\nDensenet as-is model performance: ~0.92 validation accuracy.\nDensenet fully unfrozen model performance: ~0.92 validation accuracy.\nDensenet partially frozen model performance: ~0.93 validation accuracy.\n\nAlthough additional fine-tuning is possible, we will likely get diminishing returns.\nIt is very interesting that the partially frozen model performed better than the fully unfrozen model. This suggests that the default weights of the densenet model are better than the weights we are creating as a result of training on our dataset. This makes sense because those default weights were likely created through much more training on much larger datasets. It is impressive that those default weights perform so well on our specific case even though they weren’t created with classifying cat breeds in mind (it may have been part of its training, but definitely not the entire focus)."
  },
  {
    "objectID": "posts/guide/guide.html",
    "href": "posts/guide/guide.html",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "",
    "text": "Welcome to the beginner’s guide to Supervised Machine Learning!\nIn this guide, I will walk you through complete workflow for using supervised machine learning to solve a real-world problem. Keep in mind that Supervised Machine Learning is a vast sea of knowledge that will take more than this humble little guide to fully master. However, after reading through (and following along!) this guide, I believe you will have built a solid foundation that will make you feel confident about approaching future problems using ML.\nFigure 1: A set of questions to guide you through the ML workflow."
  },
  {
    "objectID": "posts/guide/guide.html#problem-definition",
    "href": "posts/guide/guide.html#problem-definition",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "1. Problem Definition",
    "text": "1. Problem Definition\nBefore diving into data cleaning and modeling, we first have to validate whether ot not our problem is actually an ML problem. Generally, if you answer is yes to the following questions, then you have a ML problem:\n\nDoes the problem require reaching a certain output based on a set of inputs?\n\nIf not, it is unlikely that you will be able to build and train a ML model to achieve the desired result. Keep in mind that this doesn’t necessarily mean you should give up, but rather look for ways to reframe your perspective on the problem to make it more suitable for ML. For example, instead of asking “How can I increase the number of subscribers to my blog”, ask “What features of my blog are most associated with a change in the number of subscribers?”. These “Features” can be any data input you have access to that you believe may impact your result. If your problem is more general rather than personal, you may find the data your looking for in one of the plethora of open data source on the internet (e.g., https://www.kaggle.com/datasets).\n\nDo we have access to a reliable set of data that will help us reach our goal?\n\nIf not, you may attempt to manually collect the required data yourself (keep in mind that the quality of your results will depend greatly on the quality and size of your data). If you cannot find a way to access or collect the data, then I’m afraid no ML model will be able to help you.\n\nIs the data so large that a human being cannot effectively read it and use it to solve the problem?\n\nIf not, you are likely better off manually looking for patterns and using simple calculations to reach your goals. As ML models require a large amount of data to be effective.\n\n\nCongratulations! If you’ve reached this point that means you have already broken down your problem into a set of inputs (features) that influence a particular output (target). And you’ve determined a reliable source of data that will be used to train your model, and you’ve (roughly) determined that your data set is large enough to be used in ML."
  },
  {
    "objectID": "posts/guide/guide.html#preparing-our-workspace",
    "href": "posts/guide/guide.html#preparing-our-workspace",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "2. Preparing our workspace",
    "text": "2. Preparing our workspace\nIn this section, we simply want to mindfully prepare a workspace within which we will perform the majority of our analysis work. Although there are many different pieces of software that we can use for this, in this guide we will be using the Pandas package in the Python programming language using VS Code:\n\nInstall Python: https://www.python.org/downloads/\n\nPython is a programming language that houses numerous packages that contain handy tools that will help us perform our analysis. It is the most popular programming language for ML purposes.\nInstall the “conda” package to manage your environments.\n\nInstall VS Code: https://code.visualstudio.com/download\n\nVS Code is the world’s most popular integrated development environment (IDE).\nI also recommend you install the Jupyter extension within VS Code.\n\nCreate a folder that will store all of your analysis files.\nCreate a new conda environment to host the packages you will use for your analysis.\n\nInstall the “pandas” package for data manipulation.\ninstall the “sklearn” package which contains an assortment of ML models and utilities.\nInstall the “matplotlib” package for data visualization. (“altair” is a great option as well).\nFeel free to install any additional packages that you believe will be useful for your analysis."
  },
  {
    "objectID": "posts/guide/guide.html#data-collection",
    "href": "posts/guide/guide.html#data-collection",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "3. Data Collection",
    "text": "3. Data Collection\nAlthough we have already determined a source for our data, we still need to transform our data into a tabular structure that makes it easy to feed into an ML model. Here is a list of common data formats and some ways you can transform them into the structure we need:\n\nTabular Data (.CSV, .TSV, .XLS, .XLSX, etc.) - No transformation need. You’re good to go into the next part!\n\nRelational Data (MySQL, PostgreSQL, etc.) - Use SQL connector libraries such as “mysql-connector-python” for MySQL or “psycopg2” for PostgreSQL.\n\nNon-relational Data (NoSQL, MongoDB, etc.) - Use NoSQL connector libraries such as “pymongo” for MongoDB.\n\nMedia (Images, Videos, Audio) - This type of data is a bit tricky and has to be handled differently. For now it is out of the scope of this guide.\n\nSample code :\nimport pandas as pd\n\nraw_data = \"data/2023_Property_Tax_Assessment.csv\"\nhousing_df = pd.read_csv(raw_data)"
  },
  {
    "objectID": "posts/guide/guide.html#data-cleaning-and-splitting",
    "href": "posts/guide/guide.html#data-cleaning-and-splitting",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "4. Data Cleaning and Splitting",
    "text": "4. Data Cleaning and Splitting\n\nEnsure your data is tidy:\n\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\nSplit your data into training and test sets.\n\nA generally good split is 80% training data and 20% test data.\nDepending on the size of your data you may need to adjust the split to have more accurate test results.\n\n\nSample code :\nfrom sklearn.model_selection import train_test_split\n\ntidy_df = df.melt(id_vars=\"Name\", var_name=\"Subject\", value_name=\"Score\")\n\n# Splitting our tidy data into training and test data\ntrain_df, test_df = train_test_split(tidy_df, test_size=0.3, random_state=123)"
  },
  {
    "objectID": "posts/guide/guide.html#data-exploration",
    "href": "posts/guide/guide.html#data-exploration",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "5. Data Exploration",
    "text": "5. Data Exploration\n\nLook at the head and tail of your dataset.\nLook at the types, minimums, maximums, means, and medians of your features. (tip: you can use “describe()” to quickly analyze such metrics).\nUse visualization libraries such as “matplotlib” and “altair” to look at how your data is distributed.\n\nThis will give you an idea of what to expect from your data and you will be able to see any patters or potential issues from the distribution.\n\n\nSample code :\nimport altair as alt\n\n# Create a histogram for assessment values\nhistogram = alt.Chart(housing_df).mark_bar().encode(\n        alt.X(\"assess_2022:Q\", bin=alt.Bin(maxbins=2000), title=\"Assessment Value\").scale(domain=(0, 2000000), clamp=True),\n        alt.Y(\"count():Q\", title=\"Frequency\"),\n    ).properties(\n        title=\"Distribution of House Assessment Values (2022)\"\n    )\n\nFigure 2: An example of exploratory data analysis using a histogram."
  },
  {
    "objectID": "posts/guide/guide.html#preprocessing-and-feature-engineering",
    "href": "posts/guide/guide.html#preprocessing-and-feature-engineering",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "6. Preprocessing and Feature Engineering",
    "text": "6. Preprocessing and Feature Engineering\n\nPreprocessing refers to the process of translating a feature into a different scale or form to make it easier for our model to understand it. Here are some basic preprocessing technique that you can use depending on a features data type:\n\nNumeric features: Standard Scaler.\nCategorical features: One-Hot Encoding.\nBinary features: Represent with 0 and 1.\nText features: Bag-of-Words if syntax doesn’t matter. NLTK if syntax does matter. (This one is tricky and has many different approaches. The best approach depends on your specific problem).\nDate and time features: Extract meaningful components (e.g., year, month, day, hour, day of week). (This is another tricky one that has plenty of approaches. Depending on your problem and model the best approach may change).\nKeep in mind that there are plenty more types of features and plenty more preprocessing techniques, these examples are only meant to serve as a starting point.\n\nFeature Engineering can be tricky so feel free to skip this step if this is your first time using ML. Essentially, feature engineering uses existing features and information to create new features that describe the same information from a different angle that can be more beneficial for our model. For example, you may use a “Date of Birth” feature to create a new feature “Age” which can be easier to work with since it is a simple numeric feature rather than a date.\n\nSample code :\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# Divide features by type\ncategorical_features = ['garage', 'firepl', 'bsmt', 'bdevl']\nnumeric_features = ['meters']\n\n# Create the column transformer to preprocess features\npreprocessor = make_column_transformer(\n    (OneHotEncoder(), categorical_features),  # One-hot encode categorical columns\n    (StandardScaler(), numeric_features),  # Standardize numeric columns\n)"
  },
  {
    "objectID": "posts/guide/guide.html#feature-selection-and-model-building",
    "href": "posts/guide/guide.html#feature-selection-and-model-building",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "7. Feature Selection and Model Building",
    "text": "7. Feature Selection and Model Building\n\nAlthough we generally like feeding our model as much data as possible, more features doesn’t necessarily result in better performance. In fact, a large number of features can confuse our model and cause “overfitting” which will worsen its performance.\n\nWe have to be selective about which features to keep.\nStart by removing unique features such as IDs, phone number, etc. We want to remove these because they are unlikely to contain any valuable patterns that will benefit our model.\nRemove any other features that you think will have little or no impact on our target\nDon’t worry, you will have the opportunity to come back later and bring back or remove some more features to see how they impact your model.\n\nNow is a good time to do some research on what ML models generally work well for your type of problem. Here is a short list of well known models and when to use them:\n\nLinear Regression\n\nUse for: Predicting a continuous numerical value.\nExample: Predicting house prices based on its size, location, and number of bedrooms.\n\nLogistic Regression\n\nUse for: Binary classification problems.\nExample: Predicting whether a customer will churn.\n\nDecision Trees\n\nUse for: Interpretable models for classification or regression.\nExample: Classifying loan approvals based on credit scores, income, etc.\n\nRandom Forest\n\nUse for: Handling complex classification and regression tasks with reduced overfitting.\nExample: Predicting customer segments for marketing campaigns.\n\nGradient Boosting (e.g., XGBoost, LightGBM, CatBoost)\n\nUse for: Complex non-linear relationships and feature interactions.\nExample: Predicting loan default probabilities or rankings.\n\nSupport Vector Machines (SVM)\n\nUse for: Classification problems, especially with small or high-dimensional datasets.\nExample: Classifying emails as spam or non-spam.\n\nK-Nearest Neighbors (KNN)\n\nUse for: Simple classification or regression problems.\nExample: Recommending products based on user similarity.\n\nNaive Bayes\n\nUse for: Text classification with large datasets.\nExample: Classifying sentiment in customer reviews.\n\n\nIf you feel indecisive about which one to use, fret not, it is totally valid to find the best model for your problem using trial and error.\nKeep in mind that you can also use an “Ensemble” of a number of these models. But this is a more advanced technique that we will save for another time.\nYou also want to create a “dummy” model to act as your baseline to help you gauge your model. Examples include “DummyClassifier” and “DummyRegressor”.\n\nSample code :\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.pipeline import make_pipeline\n\nridge_pipeline = make_pipeline(preprocessor, RidgeCV())\ndummy_pipe = make_pipeline(preprocessor, DummyRegressor())"
  },
  {
    "objectID": "posts/guide/guide.html#evaluation-and-model-selection",
    "href": "posts/guide/guide.html#evaluation-and-model-selection",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "8. Evaluation and Model Selection",
    "text": "8. Evaluation and Model Selection\n\nUse cross-validation to evaluate the model you created.\nGo back and redo your feature selection and preprocessing to see how it affects your models performance.\nTry different models and compare them against each other.\nContinuously iterate through this process until you achieve a satisfactory validation score.\n\nSample code :\nfrom sklearn.model_selection import cross_validate\n\n# initialize results dictionary\ncross_val_results = {}\n\n# Perform cross-validation for ridge model\ncross_val_results[\"ridge\"] = pd.DataFrame(\n    cross_validate(ridge_pipeline, X_train, y_train, return_train_score=True)\n).agg(['mean', 'std']).round(3).T\n\n# Perform cross-validation for dummy model\ncross_val_results[\"dummy\"] = pd.DataFrame(\n    cross_validate(dummy_pipeline, X_train, y_train, return_train_score=True)\n).agg(['mean', 'std']).round(3).T\n\n\n\n\n\nridge\ndummy\n\n\n\n\nfit_time\n0.019\n0.012\n\n\nscore_time\n0.004\n0.003\n\n\ntest_score\n0.764\n0.304\n\n\ntrain_score\n0.812\n0.289\n\n\n\nFigure 3: Table showing cross validation results of our ridge and dummy models. Please note that “test_score” here actually refers to the validation score and is different from the test score that we will get from our test data later on."
  },
  {
    "objectID": "posts/guide/guide.html#test-data-and-predictions",
    "href": "posts/guide/guide.html#test-data-and-predictions",
    "title": "Supervised Machine Learning - Beginner’s Guide",
    "section": "9. Test data and Predictions",
    "text": "9. Test data and Predictions\n\nOnce you are finally satisfied with your model, test it one last time using the previously unseen “test data”\n\nKeep in mind that you can only do this once, and you should not go back and adjust your model to try and get a better test score as it will no longer be truly “unseen”. We call this the “golden rule”.\n\nCongratulations! your model is finally ready to start making predictions.\n\nThese predictions will likely be as accurate as the test score suggests.\n\nNow all that’s left is to communicate your findings and use those predictions to solve your problem.\n\n\nWell done! You are now ready to tackle your problems using Machine Learning.\n\n\nReferences\n\nDaumé III, H. (retrieved 2024). A Course in Machine Learning (CIML). Retrieved from https://ciml.info/\nMueller, A. C., & Guido, S. (2016). Introduction to Machine Learning with Python: A Guide for Data Scientists. O’Reilly Media.\nRussell, S., & Norvig, P. (2020). Artificial Intelligence: A Modern Approach (4th ed.). Pearson.\nTop IDE Index. (retrieved 2025). PYPL Popularity of Programming Languages. Retrieved from https://pypl.github.io/IDE.html"
  }
]